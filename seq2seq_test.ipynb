{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./preprocessing')\n",
    "sys.path.append('./seq2seq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from processor import Code_Intent_Pairs\n",
    "from model import Seq2Seq\n",
    "from data import get_train_loader, get_test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperP = {\n",
    "    ## training parameters\n",
    "    'batch_size' : 32,\n",
    "    'lr' : 1e-3,\n",
    "    'teacher_force_rate' : 1.0,\n",
    "    'max_epochs' : 45,\n",
    "    'lr_keep_rate' : 0.97,  # set to 1.0 to not decrease lr overtime\n",
    "    'load_pretrain_code_embed': False,\n",
    "    'freeze_embed': False,\n",
    "    \n",
    "    ## encoder architecture\n",
    "    'encoder_layers' : 2,\n",
    "    'encoder_embed_size' : 128,\n",
    "    'encoder_hidden_size' : 384,\n",
    "    'encoder_dropout_rate' : 0.3,\n",
    "    \n",
    "    ## decoder architecture\n",
    "    'decoder_layers' : 2,\n",
    "    'decoder_embed_size' : 128,\n",
    "    'decoder_hidden_size' : 384,\n",
    "    'decoder_dropout_rate' : 0.3,\n",
    "    \n",
    "    ## attn architecture\n",
    "    'attn_hidden_size' : 384,\n",
    "    \n",
    "    ## visualization\n",
    "    'print_every': 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_intent_pair = Code_Intent_Pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'vocab/'\n",
    "code_intent_pair.load_dict(path)\n",
    "special_symbols = code_intent_pair.get_special_symbols()\n",
    "word_size = code_intent_pair.get_word_size()\n",
    "code_size = code_intent_pair.get_code_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'processed_corpus/train.json'\n",
    "train_entries = code_intent_pair.load_entries(train_path)\n",
    "code_intent_pair.pad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = get_train_loader(train_entries, special_symbols, hyperP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(word_size, code_size, hyperP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if hyperP['load_pretrain_code_embed']:\n",
    "    model.decoder.embed[0].load_state_dict(torch.load('./pretrain_code_lm/embedding-1556211835.t7'))\n",
    "    if hyperP['freeze_embed']:\n",
    "        for param in model.decoder.embed[0].parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "optimizer = optim.Adam(model.parameters(), lr=hyperP['lr'])\n",
    "loss_f = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_keep_rate = hyperP['lr_keep_rate']\n",
    "if lr_keep_rate != 1.0:\n",
    "    lr_reduce_f = lambda epoch: lr_keep_rate ** epoch\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda=lr_reduce_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainloader, optimizer, loss_f, hyperP):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    loss_sum = 0\n",
    "    total_correct = 0\n",
    "    size = 0\n",
    "    print_every = hyperP['print_every']\n",
    "    \n",
    "    for i, (inp_seq, original_out_seq, padded_out_seq, out_lens) in enumerate(trainloader):\n",
    "        logits = model(inp_seq, padded_out_seq, out_lens)\n",
    "        loss = loss_f(logits, original_out_seq)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # show stats\n",
    "        loss_sum += loss.item()\n",
    "        total_loss += loss.item()\n",
    "        _, predictions = torch.max(logits, dim=1)\n",
    "        total_correct += (predictions == original_out_seq).sum()\n",
    "        size += len(original_out_seq)\n",
    "\n",
    "        if (i+1) % print_every == 0:\n",
    "            print('Train: loss:{}\\tacc:{}'.format(loss_sum/print_every, float(total_correct)/size), end='\\r')\n",
    "            loss_sum = 0\n",
    "            total_correct = 0\n",
    "            size = 0\n",
    "    print()\n",
    "    return total_loss / len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss:2.6172940731048584\tacc:0.40561285680019194\n",
      "Train: loss:2.093934726715088\tacc:0.490765171503957766\n",
      "Train: loss:1.7993767738342286\tacc:0.54065723195010864\n",
      "Train: loss:1.6287524700164795\tacc:0.5713600383785086\n",
      "Train: loss:1.509039855003357\tacc:0.58431278484048932\n",
      "Train: loss:1.395941424369812\tacc:0.61813384504677382\n",
      "Train: loss:1.2572482585906983\tacc:0.6380426960901895\n",
      "Train: loss:1.1904706418514253\tacc:0.6531542336291677\n",
      "Train: loss:1.050810205936432\tacc:0.68049892060446158\n",
      "Train: loss:0.9978546142578125\tacc:0.7011273686735429\n",
      "Train: loss:0.8884929299354554\tacc:0.7294315183497242\n",
      "Train: loss:0.7955094099044799\tacc:0.7555768769489086\n",
      "Train: loss:0.7768161058425903\tacc:0.7673302950347806\n",
      "Train: loss:0.6650876104831696\tacc:0.7970736387622931\n",
      "Train: loss:0.7694241106510162\tacc:0.7824418325737587\n",
      "Train: loss:0.7301493734121323\tacc:0.8076277284720557\n",
      "Train: loss:0.5397615402936935\tacc:0.8392899976013433\n",
      "Train: loss:0.48029122650623324\tacc:0.8615975053969777\n",
      "Train: loss:0.5293080121278763\tacc:0.85512113216598714\n",
      "Train: loss:0.4353303998708725\tacc:0.8747901175341809\n",
      "model saved\n",
      "Train: loss:0.4507370740175247\tacc:0.87383065483329342\n",
      "Train: loss:0.35012333244085314\tacc:0.8982969537059247\n",
      "Train: loss:0.3755638003349304\tacc:0.89589829695370593\n",
      "Train: loss:0.3430772230029106\tacc:0.90501319261213721\n",
      "Train: loss:0.33130779713392255\tacc:0.9134084912449029\n",
      "Train: loss:0.2922958880662918\tacc:0.92804029743343734\n",
      "Train: loss:0.3134398028254509\tacc:0.92276325257855648\n",
      "Train: loss:0.2485223278403282\tacc:0.9385943871431998\n",
      "Train: loss:0.20967528223991394\tacc:0.9469896857759654\n",
      "Train: loss:0.22614701017737388\tacc:0.9467498201007436\n",
      "model saved\n",
      "Train: loss:0.18262330889701844\tacc:0.9616214919644999\n",
      "Train: loss:0.2028140313923359\tacc:0.95130726792995928\n",
      "Train: loss:0.23457245156168938\tacc:0.9496282082034061\n",
      "Train: loss:0.21644352674484252\tacc:0.9491484768529623\n",
      "Train: loss:0.1916494235396385\tacc:0.95730390981050624\n",
      "Train: loss:0.18161090910434724\tacc:0.9599424322379467\n",
      "Train: loss:0.12869201973080635\tacc:0.9740945070760374\n",
      "Train: loss:0.14751092717051506\tacc:0.9671384024946033\n",
      "Train: loss:0.12852896824479104\tacc:0.9740945070760374\n",
      "Train: loss:0.11900041848421097\tacc:0.9791316862556968\n",
      "model saved\n",
      "Train: loss:0.08501135781407357\tacc:0.9832094027344687\n",
      "Train: loss:0.11844498328864575\tacc:0.9800911489565843\n",
      "Train: loss:0.1035301897674799\tacc:0.98009114895658435\n",
      "Train: loss:0.09766220897436143\tacc:0.9798512832813624\n",
      "Train: loss:0.07307688258588314\tacc:0.9856080594866875\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "teacher_force_rate = hyperP['teacher_force_rate']\n",
    "for e in range(hyperP['max_epochs']):\n",
    "    loss = train(model, trainloader, optimizer, loss_f, hyperP)\n",
    "    losses.append(loss)\n",
    "    if lr_keep_rate != 1.0:\n",
    "        scheduler.step()\n",
    "        \n",
    "    # change teacher force rate\n",
    "    teacher_force_rate = max(0.7, 0.99 * teacher_force_rate)\n",
    "    model.change_teacher_force_rate(teacher_force_rate)\n",
    "    \n",
    "    if e == 19:\n",
    "        model.save('model_20.t7')\n",
    "        print('model saved')\n",
    "    elif e == 29:\n",
    "        model.save('model_30.t7')\n",
    "        print('model saved')\n",
    "    elif e == 39:\n",
    "        model.save('model_40.t7')\n",
    "        print('model saved')\n",
    "    elif e == 44:\n",
    "        model.save('model_45.t7')\n",
    "        print('model saved')\n",
    "    elif e == 49:\n",
    "        model.save('model_50.t7')\n",
    "        print('model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_force_rate = 0.7\n",
    "model.change_teacher_force_rate(teacher_force_rate)\n",
    "for e in range(50):\n",
    "    loss = train(model, trainloader, optimizer, loss_f, hyperP)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    if e == 9:\n",
    "        model.save('model_60.t7')\n",
    "        print('model saved')\n",
    "    elif e == 19:\n",
    "        model.save('model_70.t7')\n",
    "        print('model saved')\n",
    "    elif e == 29:\n",
    "        model.save('model_80.t7')\n",
    "        print('model saved')\n",
    "    elif e == 39:\n",
    "        model.save('model_90.t7')\n",
    "        print('model saved')\n",
    "    elif e == 44:\n",
    "        model.save('model_95.t7')\n",
    "        print('model saved')\n",
    "    elif e == 49:\n",
    "        model.save('model_100.t7')\n",
    "        print('model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

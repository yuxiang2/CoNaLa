{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import preprocess_temp as P\n",
    "import model.parsers as M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "directory = './conala-corpus/'\n",
    "train_file = directory + 'train.json'\n",
    "test_file = directory + 'test.json'\n",
    "\n",
    "with open(train_file) as f:\n",
    "    train_data = json.load(f)\n",
    "    \n",
    "with open(test_file) as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's preprocess the data. Everything is in Preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# intent processing includes lowercase, remove punctuation'?'\n",
    "train_intent, train_codes = P.process_data(train_data)\n",
    "test_intent, test_codes = P.process_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this class is used for code2actions and actions2code\n",
    "ast_action = P.Ast_Action()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_actions = []\n",
    "\n",
    "for code in train_codes:\n",
    "    train_actions.append(ast_action.code2actions(code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_lst = P.vocab_list(train_intent, cut_freq=5)\n",
    "act_lst, token_lst = P.action_list(train_actions, cut_freq=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2num = dict(zip(word_lst, range(0,len(word_lst))))\n",
    "act2num = dict(zip(act_lst, range(0,len(act_lst))))\n",
    "token2num = dict(zip(token_lst, range(0,len(token_lst))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = P.get_train_loader(train_intent, train_actions, word2num, act2num, token2num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_loader = P.get_test_loader(test_intent, word2num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "action_index_copy = act2num[P.GenTokenAction('copy')]\n",
    "action_index_gen = act2num[P.GenTokenAction('token')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "hyperParamMap = {\n",
    "    #### General configuration ####\n",
    "    'cuda': True,      # Use gpu\n",
    "    'asdl_file': '',   # Path to ASDL grammar specification\n",
    "    'mode': 'train',   # train or test\n",
    "\n",
    "    #### Modularized configuration ####\n",
    "    'parser': 'default_parser',  # which parser model to use\n",
    "\n",
    "    #### Model configuration ####\n",
    "    'lstm': 'lstm',    # Type of LSTM used, currently only standard LSTM cell is supported\n",
    "\n",
    "    #### Embedding sizes ####\n",
    "    'embed_size': 128,         # Size of word embeddings\n",
    "    'action_embed_size': 128,  # Size of ApplyRule/GenToken action embeddings\n",
    "    'field_embed_size': 64,    # Embedding size of ASDL fields\n",
    "    'type_embed_size': 64,     # Embeddings ASDL types\n",
    "\n",
    "    #### Hidden sizes ####\n",
    "    'hidden_size': 256,        # Size of LSTM hidden states\n",
    "    'ptrnet_hidden_dim': 32,   # Hidden dimension used in pointer network\n",
    "    'att_vec_size': 256,       # Size of attentional vector\n",
    "\n",
    "    #### readout layer ####\n",
    "    'no_query_vec_to_action_map': False,    # Do not use additional linear layer to transform the attentional vector for computing action probabilities\n",
    "    'readout': 'identity',                  # Type of activation if using additional linear layer\n",
    "    'query_vec_to_action_diff_map': False,  # Use different linear mapping \n",
    "\n",
    "    #### parent information switch for decoder LSTM ####\n",
    "    'no_parent_production_embed': False,    # Do not use embedding of parent ASDL production to update decoder LSTM state\n",
    "    'no_parent_field_embed': False,         # Do not use embedding of parent field to update decoder LSTM state\n",
    "    'no_parent_field_type_embed': False,    # Do not use embedding of the ASDL type of parent field to update decoder LSTM state\n",
    "    'no_parent_state': True,                # Do not use the parent hidden state to update decoder LSTM state\n",
    "    'no_input_feed': False,                 # Do not use input feeding in decoder LSTM\n",
    "    'no_copy': False,                       # Do not use copy mechanism\n",
    "\n",
    "    #### Training ####\n",
    "    'vocab': '',                            # Path of the serialized vocabulary\n",
    "    'train_file': '',                       # path to the training target file\n",
    "    'dev_file': '',                         # path to the dev source file\n",
    "    'batch_size': 10,                       # Batch size\n",
    "    'dropout': 0.,                          # dropout rate\n",
    "    'word_dropout': 0.,                     # Word dropout rate\n",
    "    'decoder_word_dropout': 0.,             # Word dropout rate on decoder\n",
    "    'primitive_token_label_smoothing': 0.0, # Apply label smoothing when predicting primitive tokens\n",
    "    'src_token_label_smoothing': 0.0,       # Apply label smoothing in reconstruction model when predicting source tokens\n",
    "    'negative_sample_type': 'best',         # \n",
    "\n",
    "    #### training schedule details ####\n",
    "    'valid_metric': 'acc',                # Metric used for validation\n",
    "    'valid_every_epoch': 1,               # Perform validation every x epoch\n",
    "    'log_every': 10,                      # Log training statistics every n iterations\n",
    "    'save_to': 'model',                   # Save trained model to\n",
    "    'save_all_models': False,             # Save all intermediate checkpoints\n",
    "    'patience': 5,                        # Training patience\n",
    "    'max_num_trial': 10,                  # Stop training after x number of trials\n",
    "    'glorot_init': False,                 # Use glorot initialization\n",
    "    'clip_grad': 5.,                      # Clip gradients\n",
    "    'max_epoch': 10,                      # Maximum number of training epoches\n",
    "    'optimizer': 'Adam',                  # optimizer\n",
    "    'lr': 0.001,                          # Learning rate\n",
    "    'lr_decay': 0.5,                      # decay learning rate if the validation performance drops\n",
    "    'lr_decay_after_epoch': 0,            # Decay learning rate after x epoch\n",
    "    'decay_lr_every_epoch': False,        # force to decay learning rate after each epoch\n",
    "    'reset_optimizer': False,             # Whether to reset optimizer when loading the best checkpoint\n",
    "    'verbose': False,                     # Verbose mode\n",
    "\n",
    "    #### decoding/validation/testing ####\n",
    "    'load_model': None,                   # Load a pre-trained model\n",
    "    'beam_size': 5,                       # Beam size for beam search\n",
    "    'decode_max_time_step': 100,          # Maximum number of time steps used in decoding and sampling\n",
    "    'sample_size': 5,                     # Sample size\n",
    "    'test_file': '',                      # Path to the test file\n",
    "    'save_decode_to': None,               # Save decoding results to file\n",
    "}\n",
    "\n",
    "HyperParams = namedtuple('HyperParams', list(hyperParamMap.keys()), verbose=False)\n",
    "hyperParams = HyperParams(**hyperParamMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = M.Model(hyperParams, action_size=len(act_lst), token_size=len(token_lst), word_size=len(word_lst), \n",
    "                      action_index_copy=action_index_copy, action_index_gen=action_index_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 3e-4)\n",
    "lossFunc = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cloud\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:21: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action loss: 4.318506553986384\n",
      "Copy loss: 2.7684662342071533\n",
      "Token loss: 5.924683570861816\n",
      "GenToken total loss: 8.69314956665039\n",
      "Action loss: 3.3948511498305707\n",
      "Copy loss: 2.645263195037842\n",
      "Token loss: 5.739128112792969\n",
      "GenToken total loss: 8.384391784667969\n",
      "Action loss: 2.327069810590407\n",
      "Copy loss: 2.6278035640716553\n",
      "Token loss: 4.679891109466553\n",
      "GenToken total loss: 7.307694435119629\n",
      "Action loss: 1.9776235216942923\n",
      "Copy loss: 2.4266655445098877\n",
      "Token loss: 4.631178855895996\n",
      "GenToken total loss: 7.057844161987305\n",
      "Action loss: 1.5254496265981778\n",
      "Copy loss: 2.1510934829711914\n",
      "Token loss: 4.460443019866943\n",
      "GenToken total loss: 6.611536502838135\n",
      "Action loss: 1.3542082856396551\n",
      "Copy loss: 2.4200665950775146\n",
      "Token loss: 4.084047794342041\n",
      "GenToken total loss: 6.504114151000977\n",
      "Action loss: 1.6463950478807117\n",
      "Copy loss: 2.3245389461517334\n",
      "Token loss: 4.207257270812988\n",
      "GenToken total loss: 6.531796455383301\n",
      "Action loss: 1.4776575757490935\n",
      "Copy loss: 1.969358205795288\n",
      "Token loss: 4.503574848175049\n",
      "GenToken total loss: 6.472932815551758\n",
      "Action loss: 1.2240379183697572\n",
      "Copy loss: 2.3852953910827637\n",
      "Token loss: 4.077551364898682\n",
      "GenToken total loss: 6.462846755981445\n",
      "Action loss: 1.2331537971734359\n",
      "Copy loss: 2.2899954319000244\n",
      "Token loss: 3.877525568008423\n",
      "GenToken total loss: 6.167520999908447\n",
      "Action loss: 1.3563321138830666\n",
      "Copy loss: 2.2605299949645996\n",
      "Token loss: 3.8671441078186035\n",
      "GenToken total loss: 6.127674102783203\n",
      "Action loss: 1.5629532526540595\n",
      "Copy loss: 2.303933620452881\n",
      "Token loss: 4.124455451965332\n",
      "GenToken total loss: 6.428389072418213\n",
      "Action loss: 1.552152261618627\n",
      "Copy loss: 2.335392951965332\n",
      "Token loss: 4.440438270568848\n",
      "GenToken total loss: 6.77583122253418\n",
      "Action loss: 1.3590793428410675\n",
      "Copy loss: 2.1785197257995605\n",
      "Token loss: 4.184078693389893\n",
      "GenToken total loss: 6.362598419189453\n",
      "Action loss: 1.2266216241934276\n",
      "Copy loss: 2.0750350952148438\n",
      "Token loss: 4.233633041381836\n",
      "GenToken total loss: 6.30866813659668\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'epoch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-bb69bacf35d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mreport_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreport_examples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'[Epoch %d] epoch elapsed %ds'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mepoch_begin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'epoch' is not defined"
     ]
    }
   ],
   "source": [
    "epoch_begin = time.time()\n",
    "\n",
    "for batch_ind, x in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    (action_logits, action_labels), (copy_logits, copy_labels), (token_logits, token_labels) = model(x)\n",
    "    \n",
    "    loss1 = lossFunc(action_logits, action_labels)\n",
    "    loss2 = torch.DoubleTensor([0.0])\n",
    "    if len(copy_logits) > 0:\n",
    "        loss2 = lossFunc(copy_logits, copy_labels)\n",
    "    loss3 = torch.DoubleTensor([0.0])\n",
    "    if len(token_logits) > 0:\n",
    "        loss3 = lossFunc(token_logits, token_labels)\n",
    "\n",
    "    total_loss = loss1 + loss2.double() + loss3.double()\n",
    "    total_loss.backward()\n",
    "\n",
    "    # clip gradient\n",
    "    if hyperParams.clip_grad > 0.:\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm(model.parameters(), hyperParams.clip_grad)\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if batch_ind % 10 == 0:\n",
    "        print(\"Action loss: {}\".format(loss1.data))\n",
    "        print(\"Copy loss: {}\".format(loss2.data))\n",
    "        print(\"Token loss: {}\".format(loss3.data))\n",
    "        print(\"GenToken total loss: {}\".format(loss2.data + loss3.data))\n",
    "        report_loss = report_examples = 0.\n",
    "\n",
    "print('[Epoch %d] epoch elapsed %ds' % (epoch, time.time() - epoch_begin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

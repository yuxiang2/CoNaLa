{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import json_lines\n",
    "import preprocess_temp as P\n",
    "import model.parsers as M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './conala-corpus/'\n",
    "train_file = directory + 'train.json'\n",
    "test_file = directory + 'test.json'\n",
    "\n",
    "with open(train_file) as f:\n",
    "    train_data = json.load(f)\n",
    "    \n",
    "with open(test_file) as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mine_file = directory + 'mined.jsonl'\n",
    "mine_data = []\n",
    "with open(mine_file) as f:\n",
    "    mine_data = [line for line in json_lines.reader(f) if line['prob'] > 0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's preprocess the data. Everything is in Preprocess.py\n",
    "### Adding mined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intent processing includes lowercase, remove punctuation'?'\n",
    "train_intent, train_codes = P.process_data(train_data)\n",
    "test_intent, test_codes = P.process_data(test_data)\n",
    "mine_intent, mine_codes = P.process_data(mine_data, mine=True)\n",
    "train_intent.extend(mine_intent)\n",
    "train_codes.extend(mine_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this class is used for code2actions and actions2code\n",
    "ast_action = P.Ast_Action()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_actions = []\n",
    "\n",
    "for code in train_codes:\n",
    "    train_actions.append(ast_action.code2actions(code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_lst = P.vocab_list(train_intent, cut_freq=2)\n",
    "act_lst, token_lst = P.action_list(train_actions, cut_freq=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2num = dict(zip(word_lst, range(0,len(word_lst))))\n",
    "act2num = dict(zip(act_lst, range(0,len(act_lst))))\n",
    "token2num = dict(zip(token_lst, range(0,len(token_lst))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = P.get_train_loader(train_intent, train_actions, word2num, act2num, token2num, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_loader = P.get_test_loader(test_intent, word2num, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "action_index_copy = act2num[P.GenTokenAction('copy')]\n",
    "action_index_gen = act2num[P.GenTokenAction('token')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "hyperParamMap = {\n",
    "    #### General configuration ####\n",
    "    'cuda': True,      # Use gpu\n",
    "    'mode': 'train',   # train or test\n",
    "\n",
    "    #### Embedding sizes ####\n",
    "    'embed_size': 128,         # Size of word embeddings\n",
    "    'action_embed_size': 128,  # Size of ApplyRule/GenToken action embeddings\n",
    "    'field_embed_size': 64,    # Embedding size of ASDL fields\n",
    "    'type_embed_size': 64,     # Embeddings ASDL types\n",
    "\n",
    "    #### Decoding sizes ####\n",
    "    'hidden_size': 256,        # Size of LSTM hidden states\n",
    "\n",
    "    #### training schedule details ####\n",
    "    'valid_metric': 'acc',                # Metric used for validation\n",
    "    'valid_every_epoch': 1,               # Perform validation every x epoch\n",
    "    'log_every': 10,                      # Log training statistics every n iterations\n",
    "    'save_to': 'model',                   # Save trained model to\n",
    "    'clip_grad': 5.,                      # Clip gradients\n",
    "    'max_epoch': 10,                      # Maximum number of training epoches\n",
    "    'optimizer': 'Adam',                  # optimizer\n",
    "    'lr': 0.001,                          # Learning rate\n",
    "    'lr_decay': 0.5,                      # decay learning rate if the validation performance drops\n",
    "    'verbose': False,                     # Verbose mode\n",
    "\n",
    "    #### decoding/validation/testing ####\n",
    "    'load_model': None,                   # Load a pre-trained model\n",
    "    'beam_size': 1,                       # Beam size for beam search\n",
    "    'decode_max_time_step': 100,          # Maximum number of time steps used in decoding and sampling\n",
    "    'sample_size': 5,                     # Sample size\n",
    "    'test_file': '',                      # Path to the test file\n",
    "    'save_decode_to': None,               # Save decoding results to file\n",
    "}\n",
    "\n",
    "HyperParams = namedtuple('HyperParams', list(hyperParamMap.keys()), verbose=False)\n",
    "hyperParams = HyperParams(**hyperParamMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = M.Model(hyperParams, action_size=len(act_lst), token_size=len(token_lst), word_size=len(word_lst), \n",
    "                      action_index_copy=action_index_copy, action_index_gen=action_index_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "lossFunc = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------epoch 0 batch 9-----------------------------\n",
      "Action loss: 1.8443992640952438\n",
      "Copy loss: 2.6568801403045654\n",
      "Token loss: 6.890439510345459\n",
      "--------------------------epoch 0 batch 19-----------------------------\n",
      "Action loss: 1.4655785435146331\n",
      "Copy loss: 2.672477960586548\n",
      "Token loss: 6.080912113189697\n",
      "--------------------------epoch 0 batch 29-----------------------------\n",
      "Action loss: 1.3251404033556944\n",
      "Copy loss: 2.1825685501098633\n",
      "Token loss: 6.080993175506592\n",
      "--------------------------epoch 0 batch 39-----------------------------\n",
      "Action loss: 1.387730495106048\n",
      "Copy loss: 2.527622699737549\n",
      "Token loss: 6.132447242736816\n",
      "--------------------------epoch 0 batch 49-----------------------------\n",
      "Action loss: 1.4603556854743915\n",
      "Copy loss: 2.5043530464172363\n",
      "Token loss: 6.223443984985352\n",
      "--------------------------epoch 0 batch 59-----------------------------\n",
      "Action loss: 1.1725416518113112\n",
      "Copy loss: 2.4335925579071045\n",
      "Token loss: 6.008927345275879\n",
      "--------------------------epoch 0 batch 69-----------------------------\n",
      "Action loss: 1.2851680634805742\n",
      "Copy loss: 2.575082540512085\n",
      "Token loss: 5.7473835945129395\n",
      "--------------------------epoch 0 batch 79-----------------------------\n",
      "Action loss: 1.2532065799233185\n",
      "Copy loss: 2.678736448287964\n",
      "Token loss: 5.6281280517578125\n",
      "--------------------------epoch 0 batch 89-----------------------------\n",
      "Action loss: 1.2952278522519753\n",
      "Copy loss: 2.5016369819641113\n",
      "Token loss: 5.8176727294921875\n",
      "--------------------------epoch 0 batch 99-----------------------------\n",
      "Action loss: 1.144563685208471\n",
      "Copy loss: 2.1910836696624756\n",
      "Token loss: 6.262918949127197\n",
      "--------------------------epoch 0 batch 109-----------------------------\n",
      "Action loss: 0.997509440100714\n",
      "Copy loss: 2.4046151638031006\n",
      "Token loss: 6.0464701652526855\n",
      "--------------------------epoch 0 batch 119-----------------------------\n",
      "Action loss: 0.86630243399089\n",
      "Copy loss: 2.3723504543304443\n",
      "Token loss: 6.378785133361816\n",
      "--------------------------epoch 0 batch 129-----------------------------\n",
      "Action loss: 0.9483864672625079\n",
      "Copy loss: 2.2763352394104004\n",
      "Token loss: 5.656800746917725\n",
      "--------------------------epoch 0 batch 139-----------------------------\n",
      "Action loss: 0.8404419059930984\n",
      "Copy loss: 2.502379894256592\n",
      "Token loss: 5.5237040519714355\n",
      "--------------------------epoch 0 batch 149-----------------------------\n",
      "Action loss: 1.160822432539005\n",
      "Copy loss: 2.3960161209106445\n",
      "Token loss: 5.702807426452637\n",
      "epoch elapsed 338s\n",
      "--------------------------epoch 1 batch 9-----------------------------\n",
      "Action loss: 1.0145956083954313\n",
      "Copy loss: 2.3182523250579834\n",
      "Token loss: 5.346702575683594\n",
      "--------------------------epoch 1 batch 19-----------------------------\n",
      "Action loss: 0.9917798092395853\n",
      "Copy loss: 2.246892213821411\n",
      "Token loss: 5.441562652587891\n",
      "--------------------------epoch 1 batch 29-----------------------------\n",
      "Action loss: 0.7621781969884268\n",
      "Copy loss: 1.8685470819473267\n",
      "Token loss: 5.145484924316406\n",
      "--------------------------epoch 1 batch 39-----------------------------\n",
      "Action loss: 0.8365311201450986\n",
      "Copy loss: 2.04807186126709\n",
      "Token loss: 5.727267265319824\n",
      "--------------------------epoch 1 batch 49-----------------------------\n",
      "Action loss: 0.8957399671206079\n",
      "Copy loss: 2.3188347816467285\n",
      "Token loss: 5.01727294921875\n",
      "--------------------------epoch 1 batch 59-----------------------------\n",
      "Action loss: 0.8535203132794937\n",
      "Copy loss: 1.8513262271881104\n",
      "Token loss: 4.571493625640869\n",
      "--------------------------epoch 1 batch 69-----------------------------\n",
      "Action loss: 0.8263757369688357\n",
      "Copy loss: 2.274060010910034\n",
      "Token loss: 5.074428558349609\n",
      "--------------------------epoch 1 batch 79-----------------------------\n",
      "Action loss: 0.8973138230810968\n",
      "Copy loss: 2.1507163047790527\n",
      "Token loss: 4.896987438201904\n",
      "--------------------------epoch 1 batch 89-----------------------------\n",
      "Action loss: 0.7192601525876559\n",
      "Copy loss: 2.2941465377807617\n",
      "Token loss: 5.370503902435303\n",
      "--------------------------epoch 1 batch 99-----------------------------\n",
      "Action loss: 0.748433922025695\n",
      "Copy loss: 2.199251174926758\n",
      "Token loss: 5.078794002532959\n",
      "--------------------------epoch 1 batch 109-----------------------------\n",
      "Action loss: 0.5951424696586548\n",
      "Copy loss: 1.9217872619628906\n",
      "Token loss: 5.088832378387451\n",
      "--------------------------epoch 1 batch 119-----------------------------\n",
      "Action loss: 0.7236681973206623\n",
      "Copy loss: 1.9179950952529907\n",
      "Token loss: 5.456306457519531\n",
      "--------------------------epoch 1 batch 129-----------------------------\n",
      "Action loss: 0.6142954255624458\n",
      "Copy loss: 1.8531303405761719\n",
      "Token loss: 5.002501010894775\n",
      "--------------------------epoch 1 batch 139-----------------------------\n",
      "Action loss: 0.9492648985136568\n",
      "Copy loss: 1.9689710140228271\n",
      "Token loss: 4.768534183502197\n",
      "--------------------------epoch 1 batch 149-----------------------------\n",
      "Action loss: 0.6783960472915215\n",
      "Copy loss: 1.7994141578674316\n",
      "Token loss: 4.784603118896484\n",
      "epoch elapsed 697s\n",
      "--------------------------epoch 2 batch 9-----------------------------\n",
      "Action loss: 0.9011516204502685\n",
      "Copy loss: 1.8283352851867676\n",
      "Token loss: 5.2833380699157715\n",
      "--------------------------epoch 2 batch 19-----------------------------\n",
      "Action loss: 0.8318242987582732\n",
      "Copy loss: 1.7241761684417725\n",
      "Token loss: 4.991693496704102\n",
      "--------------------------epoch 2 batch 29-----------------------------\n",
      "Action loss: 0.840996868950838\n",
      "Copy loss: 1.8263027667999268\n",
      "Token loss: 4.077769756317139\n",
      "--------------------------epoch 2 batch 39-----------------------------\n",
      "Action loss: 0.8590470024988663\n",
      "Copy loss: 2.2467620372772217\n",
      "Token loss: 4.964622974395752\n",
      "--------------------------epoch 2 batch 49-----------------------------\n",
      "Action loss: 0.7881449197701487\n",
      "Copy loss: 1.6396735906600952\n",
      "Token loss: 4.603027820587158\n",
      "--------------------------epoch 2 batch 59-----------------------------\n",
      "Action loss: 0.7044475851246791\n",
      "Copy loss: 2.044529676437378\n",
      "Token loss: 5.05573034286499\n",
      "--------------------------epoch 2 batch 69-----------------------------\n",
      "Action loss: 0.6851963013988769\n",
      "Copy loss: 1.9474472999572754\n",
      "Token loss: 4.414978981018066\n",
      "--------------------------epoch 2 batch 79-----------------------------\n",
      "Action loss: 0.701094072383061\n",
      "Copy loss: 1.8416508436203003\n",
      "Token loss: 4.298153400421143\n",
      "--------------------------epoch 2 batch 89-----------------------------\n",
      "Action loss: 0.5885802839010826\n",
      "Copy loss: 1.7216793298721313\n",
      "Token loss: 4.397700309753418\n",
      "--------------------------epoch 2 batch 99-----------------------------\n",
      "Action loss: 0.5975944847971716\n",
      "Copy loss: 1.6338506937026978\n",
      "Token loss: 4.091222763061523\n",
      "--------------------------epoch 2 batch 109-----------------------------\n",
      "Action loss: 0.7346377414753668\n",
      "Copy loss: 1.6453242301940918\n",
      "Token loss: 4.493913650512695\n",
      "--------------------------epoch 2 batch 119-----------------------------\n",
      "Action loss: 0.5360774979320972\n",
      "Copy loss: 1.6883597373962402\n",
      "Token loss: 4.617671966552734\n",
      "--------------------------epoch 2 batch 129-----------------------------\n",
      "Action loss: 0.5209862237110501\n",
      "Copy loss: 2.0880374908447266\n",
      "Token loss: 3.6956348419189453\n",
      "--------------------------epoch 2 batch 139-----------------------------\n",
      "Action loss: 0.4084667688642215\n",
      "Copy loss: 2.102668523788452\n",
      "Token loss: 4.939703941345215\n",
      "--------------------------epoch 2 batch 149-----------------------------\n",
      "Action loss: 0.6237750007389637\n",
      "Copy loss: 1.9817990064620972\n",
      "Token loss: 4.1189351081848145\n",
      "epoch elapsed 1059s\n",
      "--------------------------epoch 3 batch 9-----------------------------\n",
      "Action loss: 0.4658453818797169\n",
      "Copy loss: 1.8194539546966553\n",
      "Token loss: 4.287525177001953\n",
      "--------------------------epoch 3 batch 19-----------------------------\n",
      "Action loss: 0.523480514894105\n",
      "Copy loss: 1.7939163446426392\n",
      "Token loss: 3.737729072570801\n",
      "--------------------------epoch 3 batch 29-----------------------------\n",
      "Action loss: 0.5863231835929359\n",
      "Copy loss: 1.7609381675720215\n",
      "Token loss: 4.113701343536377\n",
      "--------------------------epoch 3 batch 39-----------------------------\n",
      "Action loss: 0.618283810258533\n",
      "Copy loss: 1.915570855140686\n",
      "Token loss: 3.5960581302642822\n",
      "--------------------------epoch 3 batch 49-----------------------------\n",
      "Action loss: 0.4394348081057337\n",
      "Copy loss: 1.992499589920044\n",
      "Token loss: 3.7753982543945312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------epoch 3 batch 59-----------------------------\n",
      "Action loss: 0.5010012641765161\n",
      "Copy loss: 1.9128273725509644\n",
      "Token loss: 3.956510066986084\n",
      "--------------------------epoch 3 batch 69-----------------------------\n",
      "Action loss: 0.5656788169271464\n",
      "Copy loss: 1.8701472282409668\n",
      "Token loss: 3.7626655101776123\n",
      "--------------------------epoch 3 batch 79-----------------------------\n",
      "Action loss: 0.5741970905588857\n",
      "Copy loss: 1.9166284799575806\n",
      "Token loss: 4.339343547821045\n",
      "--------------------------epoch 3 batch 89-----------------------------\n",
      "Action loss: 0.5120145068049531\n",
      "Copy loss: 1.7828840017318726\n",
      "Token loss: 4.155537128448486\n",
      "--------------------------epoch 3 batch 99-----------------------------\n",
      "Action loss: 0.5491406587679667\n",
      "Copy loss: 1.554297924041748\n",
      "Token loss: 4.465452194213867\n",
      "--------------------------epoch 3 batch 109-----------------------------\n",
      "Action loss: 0.5226903305408511\n",
      "Copy loss: 1.9856289625167847\n",
      "Token loss: 3.973316192626953\n",
      "--------------------------epoch 3 batch 119-----------------------------\n",
      "Action loss: 0.5190574097511003\n",
      "Copy loss: 1.7855809926986694\n",
      "Token loss: 4.512563228607178\n",
      "--------------------------epoch 3 batch 129-----------------------------\n",
      "Action loss: 0.5425750689477788\n",
      "Copy loss: 1.8795585632324219\n",
      "Token loss: 3.7292380332946777\n",
      "--------------------------epoch 3 batch 139-----------------------------\n",
      "Action loss: 0.6658786511520147\n",
      "Copy loss: 2.027447462081909\n",
      "Token loss: 3.3805129528045654\n",
      "--------------------------epoch 3 batch 149-----------------------------\n",
      "Action loss: 0.6595411147942921\n",
      "Copy loss: 1.85516357421875\n",
      "Token loss: 3.9843218326568604\n",
      "epoch elapsed 1420s\n",
      "--------------------------epoch 4 batch 9-----------------------------\n",
      "Action loss: 0.5237360157632646\n",
      "Copy loss: 2.042969226837158\n",
      "Token loss: 3.860048294067383\n",
      "--------------------------epoch 4 batch 19-----------------------------\n",
      "Action loss: 0.4793057839200052\n",
      "Copy loss: 1.695520043373108\n",
      "Token loss: 3.7742397785186768\n",
      "--------------------------epoch 4 batch 29-----------------------------\n",
      "Action loss: 0.532110346418524\n",
      "Copy loss: 1.5453753471374512\n",
      "Token loss: 3.4140520095825195\n",
      "--------------------------epoch 4 batch 39-----------------------------\n",
      "Action loss: 0.6390874282061798\n",
      "Copy loss: 1.6702189445495605\n",
      "Token loss: 4.149099349975586\n",
      "--------------------------epoch 4 batch 49-----------------------------\n",
      "Action loss: 0.5946606374955584\n",
      "Copy loss: 2.4234344959259033\n",
      "Token loss: 3.814568281173706\n",
      "--------------------------epoch 4 batch 59-----------------------------\n",
      "Action loss: 0.49603576356862905\n",
      "Copy loss: 1.9647276401519775\n",
      "Token loss: 3.772981882095337\n",
      "--------------------------epoch 4 batch 69-----------------------------\n",
      "Action loss: 0.5327788864017761\n",
      "Copy loss: 1.790824294090271\n",
      "Token loss: 4.061182498931885\n",
      "--------------------------epoch 4 batch 79-----------------------------\n",
      "Action loss: 0.5998009574964175\n",
      "Copy loss: 1.9131495952606201\n",
      "Token loss: 4.09414005279541\n",
      "--------------------------epoch 4 batch 89-----------------------------\n",
      "Action loss: 0.5359486202949488\n",
      "Copy loss: 1.5871638059616089\n",
      "Token loss: 3.871350049972534\n",
      "--------------------------epoch 4 batch 99-----------------------------\n",
      "Action loss: 0.4637243119717187\n",
      "Copy loss: 1.9683903455734253\n",
      "Token loss: 3.7620251178741455\n",
      "--------------------------epoch 4 batch 109-----------------------------\n",
      "Action loss: 0.5673781617802011\n",
      "Copy loss: 1.6470818519592285\n",
      "Token loss: 3.770876884460449\n",
      "--------------------------epoch 4 batch 119-----------------------------\n",
      "Action loss: 0.5806841382268318\n",
      "Copy loss: 1.6906546354293823\n",
      "Token loss: 3.509587526321411\n",
      "--------------------------epoch 4 batch 129-----------------------------\n",
      "Action loss: 0.5733360070546363\n",
      "Copy loss: 1.620505452156067\n",
      "Token loss: 3.7028114795684814\n",
      "--------------------------epoch 4 batch 139-----------------------------\n",
      "Action loss: 0.4928054120935376\n",
      "Copy loss: 1.8018003702163696\n",
      "Token loss: 3.555182695388794\n",
      "--------------------------epoch 4 batch 149-----------------------------\n",
      "Action loss: 0.42899796377067256\n",
      "Copy loss: 1.5462313890457153\n",
      "Token loss: 3.293403148651123\n",
      "epoch elapsed 1775s\n",
      "--------------------------epoch 5 batch 9-----------------------------\n",
      "Action loss: 0.45844691995153175\n",
      "Copy loss: 1.705758810043335\n",
      "Token loss: 3.576491117477417\n",
      "--------------------------epoch 5 batch 19-----------------------------\n",
      "Action loss: 0.4376654010763287\n",
      "Copy loss: 1.7636945247650146\n",
      "Token loss: 3.6685903072357178\n",
      "--------------------------epoch 5 batch 29-----------------------------\n",
      "Action loss: 0.558717814649951\n",
      "Copy loss: 1.272125244140625\n",
      "Token loss: 3.3199164867401123\n",
      "--------------------------epoch 5 batch 39-----------------------------\n",
      "Action loss: 0.6141148154694013\n",
      "Copy loss: 1.7389875650405884\n",
      "Token loss: 3.653843402862549\n",
      "--------------------------epoch 5 batch 49-----------------------------\n",
      "Action loss: 0.45171736330029744\n",
      "Copy loss: 1.644407033920288\n",
      "Token loss: 3.6497344970703125\n",
      "--------------------------epoch 5 batch 59-----------------------------\n",
      "Action loss: 0.5311674276193673\n",
      "Copy loss: 1.8154056072235107\n",
      "Token loss: 3.7992303371429443\n",
      "--------------------------epoch 5 batch 69-----------------------------\n",
      "Action loss: 0.5294242435802986\n",
      "Copy loss: 2.2747936248779297\n",
      "Token loss: 3.5655517578125\n",
      "--------------------------epoch 5 batch 79-----------------------------\n",
      "Action loss: 0.4359365560240948\n",
      "Copy loss: 2.255659580230713\n",
      "Token loss: 3.1966874599456787\n",
      "--------------------------epoch 5 batch 89-----------------------------\n",
      "Action loss: 0.4971219751664258\n",
      "Copy loss: 1.721081018447876\n",
      "Token loss: 3.6358742713928223\n",
      "--------------------------epoch 5 batch 99-----------------------------\n",
      "Action loss: 0.5147741516177364\n",
      "Copy loss: 1.970846176147461\n",
      "Token loss: 2.8326895236968994\n",
      "--------------------------epoch 5 batch 109-----------------------------\n",
      "Action loss: 0.5314756909381522\n",
      "Copy loss: 1.4211593866348267\n",
      "Token loss: 3.4179434776306152\n",
      "--------------------------epoch 5 batch 119-----------------------------\n",
      "Action loss: 0.4865661449490904\n",
      "Copy loss: 2.095841884613037\n",
      "Token loss: 3.203075647354126\n",
      "--------------------------epoch 5 batch 129-----------------------------\n",
      "Action loss: 0.5854455169920516\n",
      "Copy loss: 1.870580792427063\n",
      "Token loss: 3.331265687942505\n",
      "--------------------------epoch 5 batch 139-----------------------------\n",
      "Action loss: 0.49052300651140784\n",
      "Copy loss: 1.6782782077789307\n",
      "Token loss: 3.34885311126709\n",
      "--------------------------epoch 5 batch 149-----------------------------\n",
      "Action loss: 0.4675647270980955\n",
      "Copy loss: 2.150420904159546\n",
      "Token loss: 3.3166732788085938\n",
      "epoch elapsed 2133s\n",
      "--------------------------epoch 6 batch 9-----------------------------\n",
      "Action loss: 0.6152336009109788\n",
      "Copy loss: 1.7475465536117554\n",
      "Token loss: 3.236994743347168\n",
      "--------------------------epoch 6 batch 19-----------------------------\n",
      "Action loss: 0.481674833666994\n",
      "Copy loss: 1.9345293045043945\n",
      "Token loss: 3.412297248840332\n",
      "--------------------------epoch 6 batch 29-----------------------------\n",
      "Action loss: 0.5029143586620874\n",
      "Copy loss: 1.6660737991333008\n",
      "Token loss: 3.517331123352051\n",
      "--------------------------epoch 6 batch 39-----------------------------\n",
      "Action loss: 0.4902258662481352\n",
      "Copy loss: 1.8186179399490356\n",
      "Token loss: 3.160170316696167\n",
      "--------------------------epoch 6 batch 49-----------------------------\n",
      "Action loss: 0.5517988533260613\n",
      "Copy loss: 1.8946770429611206\n",
      "Token loss: 3.4753456115722656\n",
      "--------------------------epoch 6 batch 59-----------------------------\n",
      "Action loss: 0.5210194881705101\n",
      "Copy loss: 1.5437432527542114\n",
      "Token loss: 3.257377862930298\n",
      "--------------------------epoch 6 batch 69-----------------------------\n",
      "Action loss: 0.5324236758025546\n",
      "Copy loss: 1.7737069129943848\n",
      "Token loss: 2.828490972518921\n",
      "--------------------------epoch 6 batch 79-----------------------------\n",
      "Action loss: 0.488320198470462\n",
      "Copy loss: 1.668450951576233\n",
      "Token loss: 3.54021954536438\n",
      "--------------------------epoch 6 batch 89-----------------------------\n",
      "Action loss: 0.441278933785021\n",
      "Copy loss: 1.6953519582748413\n",
      "Token loss: 2.9733667373657227\n",
      "--------------------------epoch 6 batch 99-----------------------------\n",
      "Action loss: 0.4854008326034812\n",
      "Copy loss: 1.7634234428405762\n",
      "Token loss: 3.5935685634613037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------epoch 6 batch 109-----------------------------\n",
      "Action loss: 0.42125199817983766\n",
      "Copy loss: 1.5742124319076538\n",
      "Token loss: 3.315479278564453\n",
      "--------------------------epoch 6 batch 119-----------------------------\n",
      "Action loss: 0.6294878539810345\n",
      "Copy loss: 1.6641607284545898\n",
      "Token loss: 2.970217704772949\n",
      "--------------------------epoch 6 batch 129-----------------------------\n",
      "Action loss: 0.4331703285719059\n",
      "Copy loss: 1.5983737707138062\n",
      "Token loss: 3.4850645065307617\n",
      "--------------------------epoch 6 batch 139-----------------------------\n",
      "Action loss: 0.40034234549109426\n",
      "Copy loss: 1.5500848293304443\n",
      "Token loss: 3.5025980472564697\n",
      "--------------------------epoch 6 batch 149-----------------------------\n",
      "Action loss: 0.5064771819906946\n",
      "Copy loss: 1.938191294670105\n",
      "Token loss: 3.549039602279663\n",
      "epoch elapsed 2498s\n",
      "--------------------------epoch 7 batch 9-----------------------------\n",
      "Action loss: 0.3955441176971877\n",
      "Copy loss: 1.6661779880523682\n",
      "Token loss: 2.9909024238586426\n",
      "--------------------------epoch 7 batch 19-----------------------------\n",
      "Action loss: 0.5857929340328407\n",
      "Copy loss: 1.7004601955413818\n",
      "Token loss: 3.0330331325531006\n",
      "--------------------------epoch 7 batch 29-----------------------------\n",
      "Action loss: 0.3666553446517056\n",
      "Copy loss: 1.6444376707077026\n",
      "Token loss: 3.1063284873962402\n",
      "--------------------------epoch 7 batch 39-----------------------------\n",
      "Action loss: 0.4402997814824365\n",
      "Copy loss: 1.5810973644256592\n",
      "Token loss: 2.7989678382873535\n",
      "--------------------------epoch 7 batch 49-----------------------------\n",
      "Action loss: 0.6511695138516758\n",
      "Copy loss: 1.6422287225723267\n",
      "Token loss: 2.9971444606781006\n",
      "--------------------------epoch 7 batch 59-----------------------------\n",
      "Action loss: 0.5307158031483025\n",
      "Copy loss: 1.528539776802063\n",
      "Token loss: 3.1714861392974854\n",
      "--------------------------epoch 7 batch 69-----------------------------\n",
      "Action loss: 0.585427009642778\n",
      "Copy loss: 1.7862344980239868\n",
      "Token loss: 2.8389689922332764\n",
      "--------------------------epoch 7 batch 79-----------------------------\n",
      "Action loss: 0.5386820996875986\n",
      "Copy loss: 1.7838817834854126\n",
      "Token loss: 3.377422571182251\n",
      "--------------------------epoch 7 batch 89-----------------------------\n",
      "Action loss: 0.3904147407469496\n",
      "Copy loss: 1.6656216382980347\n",
      "Token loss: 2.9789488315582275\n",
      "--------------------------epoch 7 batch 99-----------------------------\n",
      "Action loss: 0.37178826826397077\n",
      "Copy loss: 1.537699580192566\n",
      "Token loss: 2.7106597423553467\n",
      "--------------------------epoch 7 batch 109-----------------------------\n",
      "Action loss: 0.46216619667921494\n",
      "Copy loss: 1.7020156383514404\n",
      "Token loss: 2.6657192707061768\n",
      "--------------------------epoch 7 batch 119-----------------------------\n",
      "Action loss: 0.4798878269803913\n",
      "Copy loss: 1.5013129711151123\n",
      "Token loss: 2.9249794483184814\n",
      "--------------------------epoch 7 batch 129-----------------------------\n",
      "Action loss: 0.5634959418599695\n",
      "Copy loss: 1.8308442831039429\n",
      "Token loss: 3.3560547828674316\n",
      "--------------------------epoch 7 batch 139-----------------------------\n",
      "Action loss: 0.43126627244062593\n",
      "Copy loss: 1.6559349298477173\n",
      "Token loss: 3.227721929550171\n",
      "--------------------------epoch 7 batch 149-----------------------------\n",
      "Action loss: 0.5052761507897595\n",
      "Copy loss: 1.5434342622756958\n",
      "Token loss: 3.2096943855285645\n",
      "epoch elapsed 2864s\n",
      "--------------------------epoch 8 batch 9-----------------------------\n",
      "Action loss: 0.4701930960264363\n",
      "Copy loss: 1.646271824836731\n",
      "Token loss: 2.646758794784546\n",
      "--------------------------epoch 8 batch 19-----------------------------\n",
      "Action loss: 0.4022991253281824\n",
      "Copy loss: 1.7424712181091309\n",
      "Token loss: 2.697326898574829\n",
      "--------------------------epoch 8 batch 29-----------------------------\n",
      "Action loss: 0.4805720404124412\n",
      "Copy loss: 1.7711410522460938\n",
      "Token loss: 2.4851839542388916\n",
      "--------------------------epoch 8 batch 39-----------------------------\n",
      "Action loss: 0.43891514095995726\n",
      "Copy loss: 2.1735191345214844\n",
      "Token loss: 2.642900228500366\n",
      "--------------------------epoch 8 batch 49-----------------------------\n",
      "Action loss: 0.5331153560134507\n",
      "Copy loss: 1.8777779340744019\n",
      "Token loss: 2.7828540802001953\n",
      "--------------------------epoch 8 batch 59-----------------------------\n",
      "Action loss: 0.5061712268345068\n",
      "Copy loss: 1.789783239364624\n",
      "Token loss: 2.8318700790405273\n",
      "--------------------------epoch 8 batch 69-----------------------------\n",
      "Action loss: 0.4601474823349417\n",
      "Copy loss: 1.9061484336853027\n",
      "Token loss: 2.6726040840148926\n",
      "--------------------------epoch 8 batch 79-----------------------------\n",
      "Action loss: 0.49755205439001604\n",
      "Copy loss: 1.6710137128829956\n",
      "Token loss: 3.0149641036987305\n",
      "--------------------------epoch 8 batch 89-----------------------------\n",
      "Action loss: 0.3951894068490937\n",
      "Copy loss: 1.4305834770202637\n",
      "Token loss: 2.590189218521118\n",
      "--------------------------epoch 8 batch 99-----------------------------\n",
      "Action loss: 0.40781248076348176\n",
      "Copy loss: 1.8670035600662231\n",
      "Token loss: 2.827831268310547\n",
      "--------------------------epoch 8 batch 109-----------------------------\n",
      "Action loss: 0.3211979987420601\n",
      "Copy loss: 1.7872995138168335\n",
      "Token loss: 2.589298963546753\n",
      "--------------------------epoch 8 batch 119-----------------------------\n",
      "Action loss: 0.41727174178250237\n",
      "Copy loss: 1.6507580280303955\n",
      "Token loss: 2.6838746070861816\n",
      "--------------------------epoch 8 batch 129-----------------------------\n",
      "Action loss: 0.4351077428907006\n",
      "Copy loss: 1.8650113344192505\n",
      "Token loss: 2.842914581298828\n",
      "--------------------------epoch 8 batch 139-----------------------------\n",
      "Action loss: 0.47078424108216854\n",
      "Copy loss: 2.0585062503814697\n",
      "Token loss: 2.6691458225250244\n",
      "--------------------------epoch 8 batch 149-----------------------------\n",
      "Action loss: 0.4036370316625624\n",
      "Copy loss: 1.5442965030670166\n",
      "Token loss: 3.006183385848999\n",
      "epoch elapsed 3228s\n",
      "--------------------------epoch 9 batch 9-----------------------------\n",
      "Action loss: 0.5104469045631848\n",
      "Copy loss: 1.4626491069793701\n",
      "Token loss: 2.9892237186431885\n",
      "--------------------------epoch 9 batch 19-----------------------------\n",
      "Action loss: 0.47362595862177076\n",
      "Copy loss: 1.5495976209640503\n",
      "Token loss: 2.524601936340332\n",
      "--------------------------epoch 9 batch 29-----------------------------\n",
      "Action loss: 0.24746295470258592\n",
      "Copy loss: 1.6978908777236938\n",
      "Token loss: 2.431365728378296\n",
      "--------------------------epoch 9 batch 39-----------------------------\n",
      "Action loss: 0.44915061583197563\n",
      "Copy loss: 1.5759074687957764\n",
      "Token loss: 2.4762516021728516\n",
      "--------------------------epoch 9 batch 49-----------------------------\n",
      "Action loss: 0.43394384201186004\n",
      "Copy loss: 1.7711472511291504\n",
      "Token loss: 2.5339839458465576\n",
      "--------------------------epoch 9 batch 59-----------------------------\n",
      "Action loss: 0.3613661542440314\n",
      "Copy loss: 1.6894547939300537\n",
      "Token loss: 2.3709030151367188\n",
      "--------------------------epoch 9 batch 69-----------------------------\n",
      "Action loss: 0.451379806322197\n",
      "Copy loss: 1.7082895040512085\n",
      "Token loss: 2.546347141265869\n",
      "--------------------------epoch 9 batch 79-----------------------------\n",
      "Action loss: 0.3798437871855723\n",
      "Copy loss: 1.5597901344299316\n",
      "Token loss: 2.743089437484741\n",
      "--------------------------epoch 9 batch 89-----------------------------\n",
      "Action loss: 0.4780260866329841\n",
      "Copy loss: 1.7868292331695557\n",
      "Token loss: 2.794856071472168\n",
      "--------------------------epoch 9 batch 99-----------------------------\n",
      "Action loss: 0.42338221509186014\n",
      "Copy loss: 1.8084832429885864\n",
      "Token loss: 2.7546374797821045\n",
      "--------------------------epoch 9 batch 109-----------------------------\n",
      "Action loss: 0.4061351417445933\n",
      "Copy loss: 1.3194036483764648\n",
      "Token loss: 2.62821102142334\n",
      "--------------------------epoch 9 batch 119-----------------------------\n",
      "Action loss: 0.3760291277397465\n",
      "Copy loss: 1.5351721048355103\n",
      "Token loss: 2.615581512451172\n",
      "--------------------------epoch 9 batch 129-----------------------------\n",
      "Action loss: 0.5486476496061928\n",
      "Copy loss: 1.512152075767517\n",
      "Token loss: 2.5204272270202637\n",
      "--------------------------epoch 9 batch 139-----------------------------\n",
      "Action loss: 0.5210679206562592\n",
      "Copy loss: 1.421044111251831\n",
      "Token loss: 2.366420030593872\n",
      "--------------------------epoch 9 batch 149-----------------------------\n",
      "Action loss: 0.4782440905001019\n",
      "Copy loss: 1.8381751775741577\n",
      "Token loss: 2.551604986190796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch elapsed 3598s\n",
      "--------------------------epoch 10 batch 9-----------------------------\n",
      "Action loss: 0.416733834029793\n",
      "Copy loss: 1.6871261596679688\n",
      "Token loss: 2.3548285961151123\n",
      "--------------------------epoch 10 batch 19-----------------------------\n",
      "Action loss: 0.47623544746469143\n",
      "Copy loss: 1.7102437019348145\n",
      "Token loss: 2.5351016521453857\n",
      "--------------------------epoch 10 batch 29-----------------------------\n",
      "Action loss: 0.4244802492466977\n",
      "Copy loss: 1.7706047296524048\n",
      "Token loss: 2.585294723510742\n",
      "--------------------------epoch 10 batch 39-----------------------------\n",
      "Action loss: 0.35098312124400005\n",
      "Copy loss: 1.7347958087921143\n",
      "Token loss: 2.226491928100586\n",
      "--------------------------epoch 10 batch 49-----------------------------\n",
      "Action loss: 0.5529000873877636\n",
      "Copy loss: 2.013653516769409\n",
      "Token loss: 2.2179598808288574\n",
      "--------------------------epoch 10 batch 59-----------------------------\n",
      "Action loss: 0.2946308706341243\n",
      "Copy loss: 1.6628254652023315\n",
      "Token loss: 2.5219852924346924\n",
      "--------------------------epoch 10 batch 69-----------------------------\n",
      "Action loss: 0.5076300546553633\n",
      "Copy loss: 1.6134977340698242\n",
      "Token loss: 2.1634628772735596\n",
      "--------------------------epoch 10 batch 79-----------------------------\n",
      "Action loss: 0.30376603159783216\n",
      "Copy loss: 1.6604372262954712\n",
      "Token loss: 2.0951273441314697\n",
      "--------------------------epoch 10 batch 89-----------------------------\n",
      "Action loss: 0.3966497714549667\n",
      "Copy loss: 1.5717353820800781\n",
      "Token loss: 2.8223488330841064\n",
      "--------------------------epoch 10 batch 99-----------------------------\n",
      "Action loss: 0.36143631396793063\n",
      "Copy loss: 1.905985951423645\n",
      "Token loss: 2.5271809101104736\n",
      "--------------------------epoch 10 batch 109-----------------------------\n",
      "Action loss: 0.46511109699435677\n",
      "Copy loss: 1.620877742767334\n",
      "Token loss: 2.908726692199707\n",
      "--------------------------epoch 10 batch 119-----------------------------\n",
      "Action loss: 0.4450055237198553\n",
      "Copy loss: 1.6746039390563965\n",
      "Token loss: 2.371875762939453\n",
      "--------------------------epoch 10 batch 129-----------------------------\n",
      "Action loss: 0.38593788325410505\n",
      "Copy loss: 1.6252928972244263\n",
      "Token loss: 2.246518135070801\n",
      "--------------------------epoch 10 batch 139-----------------------------\n",
      "Action loss: 0.4534467621020444\n",
      "Copy loss: 1.7310631275177002\n",
      "Token loss: 2.3572516441345215\n",
      "--------------------------epoch 10 batch 149-----------------------------\n",
      "Action loss: 0.4334281127376457\n",
      "Copy loss: 1.535163164138794\n",
      "Token loss: 2.480384588241577\n",
      "epoch elapsed 3969s\n",
      "--------------------------epoch 11 batch 9-----------------------------\n",
      "Action loss: 0.42257384099688045\n",
      "Copy loss: 1.5269941091537476\n",
      "Token loss: 2.079658031463623\n",
      "--------------------------epoch 11 batch 19-----------------------------\n",
      "Action loss: 0.4292905101235195\n",
      "Copy loss: 1.8050678968429565\n",
      "Token loss: 2.39902925491333\n",
      "--------------------------epoch 11 batch 29-----------------------------\n",
      "Action loss: 0.39168730950069786\n",
      "Copy loss: 1.9450938701629639\n",
      "Token loss: 2.605163812637329\n",
      "--------------------------epoch 11 batch 39-----------------------------\n",
      "Action loss: 0.4246111291113347\n",
      "Copy loss: 1.9607937335968018\n",
      "Token loss: 2.1067912578582764\n",
      "--------------------------epoch 11 batch 49-----------------------------\n",
      "Action loss: 0.46498787717565504\n",
      "Copy loss: 1.6511270999908447\n",
      "Token loss: 2.6133804321289062\n",
      "--------------------------epoch 11 batch 59-----------------------------\n",
      "Action loss: 0.340287763546407\n",
      "Copy loss: 1.5782490968704224\n",
      "Token loss: 1.987497091293335\n",
      "--------------------------epoch 11 batch 69-----------------------------\n",
      "Action loss: 0.5824675433379735\n",
      "Copy loss: 1.7990916967391968\n",
      "Token loss: 2.3859097957611084\n",
      "--------------------------epoch 11 batch 79-----------------------------\n",
      "Action loss: 0.517662091479113\n",
      "Copy loss: 1.7822494506835938\n",
      "Token loss: 2.5184054374694824\n",
      "--------------------------epoch 11 batch 89-----------------------------\n",
      "Action loss: 0.3486042420470959\n",
      "Copy loss: 1.5211691856384277\n",
      "Token loss: 2.092815399169922\n",
      "--------------------------epoch 11 batch 99-----------------------------\n",
      "Action loss: 0.3758911209335204\n",
      "Copy loss: 2.0194389820098877\n",
      "Token loss: 2.4313745498657227\n",
      "--------------------------epoch 11 batch 109-----------------------------\n",
      "Action loss: 0.2600979879209982\n",
      "Copy loss: 2.042048931121826\n",
      "Token loss: 2.543461799621582\n",
      "--------------------------epoch 11 batch 119-----------------------------\n",
      "Action loss: 0.33777426487370266\n",
      "Copy loss: 1.7186646461486816\n",
      "Token loss: 2.4114434719085693\n",
      "--------------------------epoch 11 batch 129-----------------------------\n",
      "Action loss: 0.3681029661137259\n",
      "Copy loss: 1.4797558784484863\n",
      "Token loss: 2.0639350414276123\n",
      "--------------------------epoch 11 batch 139-----------------------------\n",
      "Action loss: 0.4472663869267341\n",
      "Copy loss: 1.7373297214508057\n",
      "Token loss: 2.699188232421875\n",
      "--------------------------epoch 11 batch 149-----------------------------\n",
      "Action loss: 0.3431776662428259\n",
      "Copy loss: 1.764844298362732\n",
      "Token loss: 2.240314483642578\n",
      "epoch elapsed 4330s\n",
      "--------------------------epoch 12 batch 9-----------------------------\n",
      "Action loss: 0.3209293692404984\n",
      "Copy loss: 1.5514649152755737\n",
      "Token loss: 2.168987512588501\n",
      "--------------------------epoch 12 batch 19-----------------------------\n",
      "Action loss: 0.39308435765336336\n",
      "Copy loss: 1.5154905319213867\n",
      "Token loss: 2.438880205154419\n",
      "--------------------------epoch 12 batch 29-----------------------------\n",
      "Action loss: 0.4674157277322577\n",
      "Copy loss: 1.2319457530975342\n",
      "Token loss: 2.0445475578308105\n",
      "--------------------------epoch 12 batch 39-----------------------------\n",
      "Action loss: 0.42775494431652344\n",
      "Copy loss: 1.6406055688858032\n",
      "Token loss: 1.9802433252334595\n",
      "--------------------------epoch 12 batch 49-----------------------------\n",
      "Action loss: 0.5696157765082853\n",
      "Copy loss: 1.9207903146743774\n",
      "Token loss: 2.0781898498535156\n",
      "--------------------------epoch 12 batch 59-----------------------------\n",
      "Action loss: 0.2729406017615782\n",
      "Copy loss: 1.5819249153137207\n",
      "Token loss: 1.904144048690796\n",
      "--------------------------epoch 12 batch 69-----------------------------\n",
      "Action loss: 0.483247601349736\n",
      "Copy loss: 1.397112488746643\n",
      "Token loss: 2.0694165229797363\n",
      "--------------------------epoch 12 batch 79-----------------------------\n",
      "Action loss: 0.4703599726764156\n",
      "Copy loss: 1.4809510707855225\n",
      "Token loss: 2.495929002761841\n",
      "--------------------------epoch 12 batch 89-----------------------------\n",
      "Action loss: 0.40052623963614326\n",
      "Copy loss: 1.5270167589187622\n",
      "Token loss: 2.2230224609375\n",
      "--------------------------epoch 12 batch 99-----------------------------\n",
      "Action loss: 0.4135958270866167\n",
      "Copy loss: 1.7156835794448853\n",
      "Token loss: 2.3678696155548096\n",
      "--------------------------epoch 12 batch 109-----------------------------\n",
      "Action loss: 0.4871208193362655\n",
      "Copy loss: 1.5371496677398682\n",
      "Token loss: 2.5503439903259277\n",
      "--------------------------epoch 12 batch 119-----------------------------\n",
      "Action loss: 0.42016402028766675\n",
      "Copy loss: 1.4946203231811523\n",
      "Token loss: 1.9534788131713867\n",
      "--------------------------epoch 12 batch 129-----------------------------\n",
      "Action loss: 0.43632700038370537\n",
      "Copy loss: 1.637603759765625\n",
      "Token loss: 2.324037551879883\n",
      "--------------------------epoch 12 batch 139-----------------------------\n",
      "Action loss: 0.35647957021832466\n",
      "Copy loss: 1.58180570602417\n",
      "Token loss: 1.9391584396362305\n",
      "--------------------------epoch 12 batch 149-----------------------------\n",
      "Action loss: 0.38711186821716026\n",
      "Copy loss: 1.7061634063720703\n",
      "Token loss: 2.263868570327759\n",
      "epoch elapsed 4696s\n",
      "--------------------------epoch 13 batch 9-----------------------------\n",
      "Action loss: 0.49810766768921266\n",
      "Copy loss: 1.2598600387573242\n",
      "Token loss: 1.8907554149627686\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-8b0e753788a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mtotal_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# clip gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \"\"\"\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "epoch_begin = time.time()\n",
    "for e in range(20):\n",
    "    for batch_ind, x in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        (action_logits, action_labels), (copy_logits, copy_labels), (token_logits, token_labels) = model(x)\n",
    "\n",
    "        loss1 = lossFunc(action_logits, action_labels)\n",
    "        loss2 = torch.DoubleTensor([0.0])\n",
    "        if len(copy_logits) > 0:\n",
    "            loss2 = lossFunc(copy_logits, copy_labels)\n",
    "        loss3 = torch.DoubleTensor([0.0])\n",
    "        if len(token_logits) > 0:\n",
    "            loss3 = lossFunc(token_logits, token_labels)\n",
    "\n",
    "        total_loss = loss1 + loss2.double() + loss3.double()\n",
    "        total_loss.backward()\n",
    "\n",
    "        # clip gradient\n",
    "        if hyperParams.clip_grad > 0.:\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), hyperParams.clip_grad)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_ind % hyperParams.log_every == hyperParams.log_every - 1:\n",
    "            print('--------------------------epoch {} batch {}-----------------------------'.format(e, batch_ind))\n",
    "            print(\"Action loss: {}\".format(loss1.data))\n",
    "            print(\"Copy loss: {}\".format(loss2.data))\n",
    "            print(\"Token loss: {}\".format(loss3.data))\n",
    "            report_loss = report_examples = 0.\n",
    "\n",
    "    print('epoch elapsed %ds' % (time.time() - epoch_begin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save((model).state_dict(), 'Parameters/frist.t7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('Parameters/frist.t7'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-1b9efb359637>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtestFile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./test_predict.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_sent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_sent_txt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0msample_hypothesis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_sent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_sent_txt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mact_lst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_lst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mast_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mast_action\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions2code\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_hypothesis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\747\\code_gene\\CoNaLa\\model\\parsers.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, intent, intent_texts, act_lst, token_lst, ast_action, decode_method, random_size)\u001b[0m\n\u001b[0;32m    485\u001b[0m                                         \u001b[0mrandom_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m                                         \u001b[0munknown_token_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munknown_token_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 487\u001b[1;33m                                         max_time_step=100)\n\u001b[0m\u001b[0;32m    488\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m                 return self.decoder.decode_evaluate_beam(intent=intent,\n",
      "\u001b[1;32m~\\Desktop\\747\\code_gene\\CoNaLa\\model\\parsers.py\u001b[0m in \u001b[0;36mdecode_evaluate_random\u001b[1;34m(self, intent, intent_text, encoder_hidden, sentence_encoding, action_index_copy, action_index_gen, act_lst, token_lst, batch_lens, ast_action, random_size, unknown_token_index, max_time_step)\u001b[0m\n\u001b[0;32m    432\u001b[0m         return self.decode_evaluate(intent, intent_text, encoder_hidden, sentence_encoding, \n\u001b[0;32m    433\u001b[0m                         \u001b[0maction_index_copy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_index_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mact_lst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_lst\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m                         batch_lens, ast_action, beam_size=1, unknown_token_index=0)\n\u001b[0m\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\747\\code_gene\\CoNaLa\\model\\parsers.py\u001b[0m in \u001b[0;36mdecode_evaluate\u001b[1;34m(self, intent, intent_text, encoder_hidden, sentence_encoding, action_index_copy, action_index_gen, act_lst, token_lst, batch_lens, ast_action, beam_size, unknown_token_index)\u001b[0m\n\u001b[0;32m    232\u001b[0m             \u001b[1;31m# decode one step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m             hiddens, att_context = self.decode_step(action_embed_tm1, hiddens, sentence_encoding,\n\u001b[1;32m--> 234\u001b[1;33m                                                     batch_lens, att_context)\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[1;31m# classify action types\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\747\\code_gene\\CoNaLa\\model\\parsers.py\u001b[0m in \u001b[0;36mdecode_step\u001b[1;34m(self, action_embed_tm1, hiddens, encoder_outputs, batch_lens, att_context)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;31m# main rnn cells\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mh_t0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell_t0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcell1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0matt_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhiddens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[0mh_t1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell_t1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcell2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh_t0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhiddens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mh_t2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell_t2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcell3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh_t1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhiddens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    711\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight_ih\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight_hh\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 713\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias_ih\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias_hh\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    714\u001b[0m         )\n\u001b[0;32m    715\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\_functions\\rnn.py\u001b[0m in \u001b[0;36mLSTMCell\u001b[1;34m(input, hidden, w_ih, w_hh, b_ih, b_hh)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mforgetgate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mforgetgate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mcellgate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcellgate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0moutgate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutgate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0mcy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mforgetgate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mcx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mingate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mcellgate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "model.eval()\n",
    "test_loader = P.get_test_loader(test_intent, word2num, batch_size=1)\n",
    "testFile = codecs.open('./test_predict.txt','w','utf-8')\n",
    "for i,(sample_sent, sample_sent_txt) in enumerate(test_loader):\n",
    "    sample_hypothesis = model.parse(sample_sent, sample_sent_txt, act_lst, token_lst, ast_action)\n",
    "    try:\n",
    "        code = ast_action.actions2code(sample_hypothesis.actions)\n",
    "    except:\n",
    "        for i in range(len(sample_hypothesis.actions)):\n",
    "            action = sample_hypothesis.actions[i]\n",
    "            if isinstance(action, P.GenTokenAction):\n",
    "                if action.token in ['list', 'reverse', 'range', 'address', 'in', 'and', 'range']:\n",
    "                    action.token = '.'\n",
    "        try:\n",
    "            code = ast_action.actions2code(sample_hypothesis.actions)\n",
    "        except:\n",
    "            code = ''\n",
    "#     print(test_intent[i])\n",
    "    testFile.write(' '.join(test_intent[i])+':'+code+'\\n')\n",
    "testFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import preprocess_temp as P\n",
    "import model.parsers as M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './conala-corpus/'\n",
    "train_file = directory + 'train.json'\n",
    "test_file = directory + 'test.json'\n",
    "\n",
    "with open(train_file) as f:\n",
    "    train_data = json.load(f)\n",
    "    \n",
    "with open(test_file) as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's preprocess the data. Everything is in Preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intent processing includes lowercase, remove punctuation'?'\n",
    "train_intent, train_codes = P.process_data(train_data)\n",
    "test_intent, test_codes = P.process_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this class is used for code2actions and actions2code\n",
    "ast_action = P.Ast_Action()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_actions = []\n",
    "\n",
    "for code in train_codes:\n",
    "    train_actions.append(ast_action.code2actions(code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_lst = P.vocab_list(train_intent, cut_freq=5)\n",
    "act_lst, token_lst = P.action_list(train_actions, cut_freq=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2num = dict(zip(word_lst, range(0,len(word_lst))))\n",
    "act2num = dict(zip(act_lst, range(0,len(act_lst))))\n",
    "token2num = dict(zip(token_lst, range(0,len(token_lst))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = P.get_train_loader(train_intent, train_actions, word2num, act2num, token2num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = P.get_test_loader(test_intent, word2num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_index_copy = act2num[P.GenTokenAction('copy')]\n",
    "action_index_gen = act2num[P.GenTokenAction('token')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "hyperParamMap = {\n",
    "    #### General configuration ####\n",
    "    'cuda': True,      # Use gpu\n",
    "    'mode': 'train',   # train or test\n",
    "\n",
    "    #### Embedding sizes ####\n",
    "    'embed_size': 128,         # Size of word embeddings\n",
    "    'action_embed_size': 128,  # Size of ApplyRule/GenToken action embeddings\n",
    "    'field_embed_size': 64,    # Embedding size of ASDL fields\n",
    "    'type_embed_size': 64,     # Embeddings ASDL types\n",
    "\n",
    "    #### Decoding sizes ####\n",
    "    'hidden_size': 256,        # Size of LSTM hidden states\n",
    "\n",
    "    #### training schedule details ####\n",
    "    'valid_metric': 'acc',                # Metric used for validation\n",
    "    'valid_every_epoch': 1,               # Perform validation every x epoch\n",
    "    'log_every': 10,                      # Log training statistics every n iterations\n",
    "    'save_to': 'model',                   # Save trained model to\n",
    "    'clip_grad': 5.,                      # Clip gradients\n",
    "    'max_epoch': 10,                      # Maximum number of training epoches\n",
    "    'optimizer': 'Adam',                  # optimizer\n",
    "    'lr': 0.001,                          # Learning rate\n",
    "    'lr_decay': 0.5,                      # decay learning rate if the validation performance drops\n",
    "    'verbose': False,                     # Verbose mode\n",
    "\n",
    "    #### decoding/validation/testing ####\n",
    "    'load_model': None,                   # Load a pre-trained model\n",
    "    'beam_size': 5,                       # Beam size for beam search\n",
    "    'decode_max_time_step': 100,          # Maximum number of time steps used in decoding and sampling\n",
    "    'sample_size': 5,                     # Sample size\n",
    "    'test_file': '',                      # Path to the test file\n",
    "    'save_decode_to': None,               # Save decoding results to file\n",
    "}\n",
    "\n",
    "HyperParams = namedtuple('HyperParams', list(hyperParamMap.keys()), verbose=False)\n",
    "hyperParams = HyperParams(**hyperParamMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = M.Model(hyperParams, action_size=len(act_lst), token_size=len(token_lst), word_size=len(word_lst), \n",
    "                      action_index_copy=action_index_copy, action_index_gen=action_index_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 3e-4)\n",
    "lossFunc = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action loss: 2.016467615584445\n",
      "Copy loss: 2.961787462234497\n",
      "Token loss: 5.116165637969971\n",
      "-------------------------------------------------------\n",
      "Action loss: 1.6260644621086704\n",
      "Copy loss: 2.511514902114868\n",
      "Token loss: 4.443061351776123\n",
      "-------------------------------------------------------\n",
      "Action loss: 1.3185282024003342\n",
      "Copy loss: 2.413918972015381\n",
      "Token loss: 3.7202351093292236\n",
      "-------------------------------------------------------\n",
      "Action loss: 1.0786224066894838\n",
      "Copy loss: 2.59718656539917\n",
      "Token loss: 3.972409963607788\n",
      "-------------------------------------------------------\n",
      "Action loss: 1.4810984135296084\n",
      "Copy loss: 2.2959377765655518\n",
      "Token loss: 4.179492473602295\n",
      "-------------------------------------------------------\n",
      "Action loss: 1.4190842994179527\n",
      "Copy loss: 2.313582181930542\n",
      "Token loss: 4.215724945068359\n",
      "-------------------------------------------------------\n",
      "Action loss: 1.419256925725419\n",
      "Copy loss: 2.427631378173828\n",
      "Token loss: 4.175936698913574\n",
      "-------------------------------------------------------\n",
      "epoch elapsed 93s\n",
      "Action loss: 1.307063017914089\n",
      "Copy loss: 2.295933961868286\n",
      "Token loss: 3.961423635482788\n",
      "-------------------------------------------------------\n",
      "Action loss: 1.4697151682557503\n",
      "Copy loss: 2.5740325450897217\n",
      "Token loss: 4.031152248382568\n",
      "-------------------------------------------------------\n",
      "Action loss: 1.4286677604727325\n",
      "Copy loss: 2.640738010406494\n",
      "Token loss: 4.363177299499512\n",
      "-------------------------------------------------------\n",
      "Action loss: 1.023597204383998\n",
      "Copy loss: 2.3836138248443604\n",
      "Token loss: 3.7093045711517334\n",
      "-------------------------------------------------------\n",
      "Action loss: 1.041770288136167\n",
      "Copy loss: 2.4748451709747314\n",
      "Token loss: 3.9848573207855225\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.7085196839767889\n",
      "Copy loss: 2.5611908435821533\n",
      "Token loss: 3.7437517642974854\n",
      "-------------------------------------------------------\n",
      "Action loss: 1.0843166797921362\n",
      "Copy loss: 2.630819320678711\n",
      "Token loss: 3.861910820007324\n",
      "-------------------------------------------------------\n",
      "epoch elapsed 180s\n",
      "Action loss: 1.3011020002918834\n",
      "Copy loss: 2.497570753097534\n",
      "Token loss: 4.134027004241943\n",
      "-------------------------------------------------------\n",
      "Action loss: 1.0220737608977613\n",
      "Copy loss: 2.497173309326172\n",
      "Token loss: 3.9846436977386475\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.9517425926110262\n",
      "Copy loss: 2.392207622528076\n",
      "Token loss: 3.64766526222229\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.8538493452087021\n",
      "Copy loss: 2.1773741245269775\n",
      "Token loss: 3.6293816566467285\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.8177265009023463\n",
      "Copy loss: 2.3075621128082275\n",
      "Token loss: 3.636258840560913\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.7643138797682576\n",
      "Copy loss: 2.4905619621276855\n",
      "Token loss: 3.6523544788360596\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.9873547311497858\n",
      "Copy loss: 2.451240062713623\n",
      "Token loss: 3.400981903076172\n",
      "-------------------------------------------------------\n",
      "epoch elapsed 278s\n",
      "Action loss: 0.9127968526375064\n",
      "Copy loss: 2.750800848007202\n",
      "Token loss: 3.29272198677063\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.7343462532356908\n",
      "Copy loss: 2.5831081867218018\n",
      "Token loss: 3.487804651260376\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.6848414138363874\n",
      "Copy loss: 2.319706916809082\n",
      "Token loss: 3.0159213542938232\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.7926309057802972\n",
      "Copy loss: 2.790433406829834\n",
      "Token loss: 3.235846996307373\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.9404588879284311\n",
      "Copy loss: 2.4652798175811768\n",
      "Token loss: 2.1475398540496826\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.9280060504074629\n",
      "Copy loss: 2.7539076805114746\n",
      "Token loss: 3.136936664581299\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.8124075728066579\n",
      "Copy loss: 2.3654069900512695\n",
      "Token loss: 2.709341049194336\n",
      "-------------------------------------------------------\n",
      "epoch elapsed 365s\n",
      "Action loss: 0.7152557216669263\n",
      "Copy loss: 2.5821056365966797\n",
      "Token loss: 2.9435627460479736\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.7497579037303468\n",
      "Copy loss: 2.349619150161743\n",
      "Token loss: 2.7606115341186523\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.6879366780840418\n",
      "Copy loss: 2.3235864639282227\n",
      "Token loss: 3.038881301879883\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.6285323977073634\n",
      "Copy loss: 2.220125913619995\n",
      "Token loss: 2.989044189453125\n",
      "-------------------------------------------------------\n",
      "Action loss: 1.0407894594279443\n",
      "Copy loss: 2.4931652545928955\n",
      "Token loss: 2.9738070964813232\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.8540698404943892\n",
      "Copy loss: 1.8659816980361938\n",
      "Token loss: 3.131580114364624\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.8047591903333216\n",
      "Copy loss: 2.1814534664154053\n",
      "Token loss: 2.3976354598999023\n",
      "-------------------------------------------------------\n",
      "epoch elapsed 452s\n",
      "Action loss: 0.7713901565113647\n",
      "Copy loss: 2.3287434577941895\n",
      "Token loss: 2.6595027446746826\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.7339675930497354\n",
      "Copy loss: 2.0496678352355957\n",
      "Token loss: 2.933856725692749\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.7307159752366675\n",
      "Copy loss: 2.002293825149536\n",
      "Token loss: 2.3144500255584717\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.6337541547154716\n",
      "Copy loss: 1.8668359518051147\n",
      "Token loss: 2.6120729446411133\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.4881313249264366\n",
      "Copy loss: 1.940348744392395\n",
      "Token loss: 2.3571460247039795\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.6571495858835231\n",
      "Copy loss: 1.613804578781128\n",
      "Token loss: 2.691079616546631\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.5471991343449836\n",
      "Copy loss: 1.785605788230896\n",
      "Token loss: 2.381530284881592\n",
      "-------------------------------------------------------\n",
      "epoch elapsed 546s\n",
      "Action loss: 0.7032196669736672\n",
      "Copy loss: 1.63334059715271\n",
      "Token loss: 2.599349021911621\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.7234988899611902\n",
      "Copy loss: 2.1146559715270996\n",
      "Token loss: 2.3743491172790527\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.5992992782819688\n",
      "Copy loss: 2.0621156692504883\n",
      "Token loss: 2.4040651321411133\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.7906717946279588\n",
      "Copy loss: 2.297983407974243\n",
      "Token loss: 2.4030978679656982\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.7065616288501287\n",
      "Copy loss: 1.8957542181015015\n",
      "Token loss: 2.3950083255767822\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.7393877796101866\n",
      "Copy loss: 2.258305788040161\n",
      "Token loss: 2.1766374111175537\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.4316728146727278\n",
      "Copy loss: 1.6802986860275269\n",
      "Token loss: 2.5774359703063965\n",
      "-------------------------------------------------------\n",
      "epoch elapsed 633s\n",
      "Action loss: 0.5496349127302063\n",
      "Copy loss: 1.8688361644744873\n",
      "Token loss: 2.358952760696411\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.5443318448815043\n",
      "Copy loss: 2.144880771636963\n",
      "Token loss: 2.3696229457855225\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.4194821509325027\n",
      "Copy loss: 2.2449703216552734\n",
      "Token loss: 1.8291927576065063\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.5593376466669769\n",
      "Copy loss: 1.7430323362350464\n",
      "Token loss: 2.3007266521453857\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.48579903598833457\n",
      "Copy loss: 2.244419574737549\n",
      "Token loss: 2.2328240871429443\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.6423920878572009\n",
      "Copy loss: 2.308206558227539\n",
      "Token loss: 2.559518575668335\n",
      "-------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action loss: 0.5962390033726052\n",
      "Copy loss: 1.9728648662567139\n",
      "Token loss: 2.480133295059204\n",
      "-------------------------------------------------------\n",
      "epoch elapsed 720s\n",
      "Action loss: 0.6383688236411884\n",
      "Copy loss: 1.690181016921997\n",
      "Token loss: 2.1056313514709473\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.6901127979819148\n",
      "Copy loss: 1.7206459045410156\n",
      "Token loss: 2.44681715965271\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.540737620880067\n",
      "Copy loss: 1.8521512746810913\n",
      "Token loss: 1.835858941078186\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.6694366436070495\n",
      "Copy loss: 1.6602026224136353\n",
      "Token loss: 2.3602547645568848\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.6037223597773073\n",
      "Copy loss: 2.2827999591827393\n",
      "Token loss: 1.8742506504058838\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.5091142660736357\n",
      "Copy loss: 2.2652297019958496\n",
      "Token loss: 2.314922332763672\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.4935991006867031\n",
      "Copy loss: 1.8383851051330566\n",
      "Token loss: 2.2237045764923096\n",
      "-------------------------------------------------------\n",
      "epoch elapsed 811s\n",
      "Action loss: 0.48779158645327214\n",
      "Copy loss: 2.086836814880371\n",
      "Token loss: 1.7038249969482422\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.6515956345408969\n",
      "Copy loss: 1.7985763549804688\n",
      "Token loss: 2.198744535446167\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.5108147016708243\n",
      "Copy loss: 2.0197091102600098\n",
      "Token loss: 2.2985386848449707\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.4040664235852151\n",
      "Copy loss: 2.241133213043213\n",
      "Token loss: 2.273211717605591\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.4552583267359077\n",
      "Copy loss: 1.8640679121017456\n",
      "Token loss: 1.8397495746612549\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.6926747751336251\n",
      "Copy loss: 1.7624763250350952\n",
      "Token loss: 2.2074811458587646\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.6007495227706733\n",
      "Copy loss: 1.569559097290039\n",
      "Token loss: 1.7199064493179321\n",
      "-------------------------------------------------------\n",
      "epoch elapsed 898s\n"
     ]
    }
   ],
   "source": [
    "epoch_begin = time.time()\n",
    "for e in range(10):\n",
    "    for batch_ind, x in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        (action_logits, action_labels), (copy_logits, copy_labels), (token_logits, token_labels) = model(x)\n",
    "\n",
    "        loss1 = lossFunc(action_logits, action_labels)\n",
    "        loss2 = torch.DoubleTensor([0.0])\n",
    "        if len(copy_logits) > 0:\n",
    "            loss2 = lossFunc(copy_logits, copy_labels)\n",
    "        loss3 = torch.DoubleTensor([0.0])\n",
    "        if len(token_logits) > 0:\n",
    "            loss3 = lossFunc(token_logits, token_labels)\n",
    "\n",
    "        total_loss = loss1 + loss2.double() + loss3.double()\n",
    "        total_loss.backward()\n",
    "\n",
    "        # clip gradient\n",
    "        if hyperParams.clip_grad > 0.:\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), hyperParams.clip_grad)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_ind % hyperParams.log_every == hyperParams.log_every - 1:\n",
    "            print(\"Action loss: {}\".format(loss1.data))\n",
    "            print(\"Copy loss: {}\".format(loss2.data))\n",
    "            print(\"Token loss: {}\".format(loss3.data))\n",
    "            print('-------------------------------------------------------')\n",
    "            report_loss = report_examples = 0.\n",
    "\n",
    "    print('epoch elapsed %ds' % (time.time() - epoch_begin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save((model).state_dict(), 'Parameters/frist.t7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('Parameters/frist.t7'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

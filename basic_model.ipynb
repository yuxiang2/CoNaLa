{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import json_lines\n",
    "import preprocess_temp as P\n",
    "import model.parsers as M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install json-lines if you don't have json_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './conala-corpus/'\n",
    "train_file = directory + 'train.json'\n",
    "test_file = directory + 'test.json'\n",
    "\n",
    "with open(train_file) as f:\n",
    "    train_data = json.load(f)\n",
    "    \n",
    "with open(test_file) as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mine_file = directory + 'mined.jsonl'\n",
    "mine_data = []\n",
    "with open(mine_file) as f:\n",
    "    mine_data = [line for line in json_lines.reader(f)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's preprocess the data. Everything is in Preprocess.py\n",
    "### Adding mined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intent processing includes lowercase, remove punctuation'?'\n",
    "train_intent, train_codes = P.process_data(train_data)\n",
    "test_intent, test_codes = P.process_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mine_intent, mine_codes = P.process_data(mine_data, mine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this class is used for code2actions and actions2code\n",
    "ast_action = P.Ast_Action()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_actions = []\n",
    "\n",
    "for code in train_codes:\n",
    "    train_actions.append(ast_action.code2actions(code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_lst = P.vocab_list(train_intent, cut_freq=5)\n",
    "act_lst, token_lst = P.action_list(train_actions, cut_freq=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ApplyRule[cmpop -> Is()],\n",
       " ApplyRule[cmpop -> NotEq()],\n",
       " ApplyRule[expr -> SetComp(expr elt, comprehension* generators)],\n",
       " ApplyRule[operator -> Add()],\n",
       " ApplyRule[operator -> FloorDiv()],\n",
       " ApplyRule[keyword -> keyword(identifier? arg, expr value)],\n",
       " ApplyRule[expr -> ListComp(expr elt, comprehension* generators)],\n",
       " ApplyRule[stmt -> Return(expr? value)],\n",
       " ApplyRule[expr -> BinOp(expr left, operator op, expr right)],\n",
       " ApplyRule[cmpop -> Lt()],\n",
       " ApplyRule[expr -> Str(string s)],\n",
       " ApplyRule[unaryop -> USub()],\n",
       " ApplyRule[expr -> Name(identifier id)],\n",
       " ApplyRule[expr -> Dict(expr* keys, expr* values)],\n",
       " ApplyRule[slice -> Index(expr value)],\n",
       " ApplyRule[expr -> NameConstant(singleton value)],\n",
       " ApplyRule[operator -> BitOr()],\n",
       " GenToken[token],\n",
       " ApplyRule[cmpop -> IsNot()],\n",
       " ApplyRule[boolop -> And()],\n",
       " ApplyRule[stmt -> Try(stmt* body, excepthandler* handlers, stmt* orelse, stmt* finalbody)],\n",
       " ApplyRule[stmt -> If(expr test, stmt* body, stmt* orelse)],\n",
       " ApplyRule[expr -> Lambda(arguments args, expr body)],\n",
       " ApplyRule[stmt -> Expr(expr value)],\n",
       " ApplyRule[arg -> arg(identifier arg, expr? annotation)],\n",
       " ApplyRule[expr -> Attribute(expr value, identifier attr)],\n",
       " ApplyRule[stmt -> Pass()],\n",
       " ApplyRule[operator -> Mod()],\n",
       " ApplyRule[stmt -> While(expr test, stmt* body, stmt* orelse)],\n",
       " Reduce,\n",
       " ApplyRule[unaryop -> UAdd()],\n",
       " ApplyRule[stmt -> Delete(expr* targets)],\n",
       " ApplyRule[arguments -> arguments(arg* args, arg? vararg, arg* kwonlyargs, expr* kw_defaults, arg? kwarg, expr* defaults)],\n",
       " ApplyRule[cmpop -> Eq()],\n",
       " ApplyRule[operator -> BitAnd()],\n",
       " ApplyRule[stmt -> With(withitem* items, stmt* body)],\n",
       " ApplyRule[stmt -> ImportFrom(identifier? module, alias* names, int? level)],\n",
       " ApplyRule[slice -> Slice(expr? lower, expr? upper, expr? step)],\n",
       " ApplyRule[expr -> Subscript(expr value, slice slice)],\n",
       " ApplyRule[cmpop -> LtE()],\n",
       " ApplyRule[expr -> GeneratorExp(expr elt, comprehension* generators)],\n",
       " GenToken[copy],\n",
       " ApplyRule[comprehension -> comprehension(expr target, expr iter, expr* ifs)],\n",
       " ApplyRule[expr -> DictComp(expr key, expr value, comprehension* generators)],\n",
       " ApplyRule[expr -> Call(expr func, expr* args, keyword* keywords)],\n",
       " ApplyRule[withitem -> withitem(expr context_expr, expr? optional_vars)],\n",
       " ApplyRule[unaryop -> Invert()],\n",
       " ApplyRule[expr -> Compare(expr left, cmpop* ops, expr* comparators)],\n",
       " ApplyRule[expr -> List(expr* elts)],\n",
       " ApplyRule[operator -> BitXor()],\n",
       " ApplyRule[cmpop -> NotIn()],\n",
       " ApplyRule[operator -> Mult()],\n",
       " ApplyRule[cmpop -> GtE()],\n",
       " ApplyRule[expr -> IfExp(expr test, expr body, expr orelse)],\n",
       " ApplyRule[stmt -> Import(alias* names)],\n",
       " ApplyRule[stmt -> FunctionDef(identifier name, arguments args, stmt* body, expr* decorator_list, expr? returns)],\n",
       " ApplyRule[operator -> LShift()],\n",
       " ApplyRule[expr -> Tuple(expr* elts)],\n",
       " ApplyRule[boolop -> Or()],\n",
       " ApplyRule[slice -> ExtSlice(slice* dims)],\n",
       " ApplyRule[stmt -> AugAssign(expr target, operator op, expr value)],\n",
       " ApplyRule[stmt -> For(expr target, expr iter, stmt* body, stmt* orelse)],\n",
       " ApplyRule[operator -> Pow()],\n",
       " ApplyRule[cmpop -> In()],\n",
       " ApplyRule[cmpop -> Gt()],\n",
       " ApplyRule[unaryop -> Not()],\n",
       " ApplyRule[excepthandler -> ExceptHandler(expr? type, identifier? name, stmt* body)],\n",
       " ApplyRule[expr -> UnaryOp(unaryop op, expr operand)],\n",
       " ApplyRule[stmt -> Raise(expr? exc, expr? cause)],\n",
       " ApplyRule[alias -> alias(identifier name, identifier? asname)],\n",
       " ApplyRule[expr -> Num(object n)],\n",
       " ApplyRule[stmt -> Assign(expr* targets, expr value)],\n",
       " ApplyRule[expr -> BoolOp(boolop op, expr* values)],\n",
       " ApplyRule[operator -> Sub()],\n",
       " ApplyRule[expr -> Set(expr* elts)],\n",
       " ApplyRule[expr -> Starred(expr value)],\n",
       " ApplyRule[operator -> Div()]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2num = dict(zip(word_lst, range(0,len(word_lst))))\n",
    "act2num = dict(zip(act_lst, range(0,len(act_lst))))\n",
    "token2num = dict(zip(token_lst, range(0,len(token_lst))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = P.get_train_loader(train_intent, train_actions, word2num, act2num, token2num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = P.get_test_loader(test_intent, word2num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_index_copy = act2num[P.GenTokenAction('copy')]\n",
    "action_index_gen = act2num[P.GenTokenAction('token')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "hyperParamMap = {\n",
    "    #### General configuration ####\n",
    "    'cuda': True,      # Use gpu\n",
    "    'mode': 'train',   # train or test\n",
    "\n",
    "    #### Embedding sizes ####\n",
    "    'embed_size': 128,         # Size of word embeddings\n",
    "    'action_embed_size': 128,  # Size of ApplyRule/GenToken action embeddings\n",
    "    'field_embed_size': 64,    # Embedding size of ASDL fields\n",
    "    'type_embed_size': 64,     # Embeddings ASDL types\n",
    "\n",
    "    #### Decoding sizes ####\n",
    "    'hidden_size': 256,        # Size of LSTM hidden states\n",
    "\n",
    "    #### training schedule details ####\n",
    "    'valid_metric': 'acc',                # Metric used for validation\n",
    "    'valid_every_epoch': 1,               # Perform validation every x epoch\n",
    "    'log_every': 30,                      # Log training statistics every n iterations\n",
    "    'save_to': 'model',                   # Save trained model to\n",
    "    'clip_grad': 5.,                      # Clip gradients\n",
    "    'max_epoch': 10,                      # Maximum number of training epoches\n",
    "    'optimizer': 'Adam',                  # optimizer\n",
    "    'lr': 0.001,                          # Learning rate\n",
    "    'lr_decay': 0.5,                      # decay learning rate if the validation performance drops\n",
    "    'verbose': False,                     # Verbose mode\n",
    "\n",
    "    #### decoding/validation/testing ####\n",
    "    'load_model': None,                   # Load a pre-trained model\n",
    "    'beam_size': 5,                       # Beam size for beam search\n",
    "    'decode_max_time_step': 100,          # Maximum number of time steps used in decoding and sampling\n",
    "    'sample_size': 5,                     # Sample size\n",
    "    'test_file': '',                      # Path to the test file\n",
    "    'save_decode_to': None,               # Save decoding results to file\n",
    "}\n",
    "\n",
    "HyperParams = namedtuple('HyperParams', list(hyperParamMap.keys()), verbose=False)\n",
    "hyperParams = HyperParams(**hyperParamMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = M.Model(hyperParams, action_size=len(act_lst), token_size=len(token_lst), word_size=len(word_lst), \n",
    "                      action_index_copy=action_index_copy, action_index_gen=action_index_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "lossFunc = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action loss: 0.7167027586575205\n",
      "Copy loss: 2.3950161933898926\n",
      "Token loss: 2.809378147125244\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.42719784815837963\n",
      "Copy loss: 2.468154191970825\n",
      "Token loss: 2.8486595153808594\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.6902822944227888\n",
      "Copy loss: 2.298081159591675\n",
      "Token loss: 2.528050184249878\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.7904933715868996\n",
      "Copy loss: 2.854611396789551\n",
      "Token loss: 2.4207215309143066\n",
      "-------------------------------------------------------\n",
      "epoch elapsed 85s\n",
      "Action loss: 0.6646689647056626\n",
      "Copy loss: 2.5164413452148438\n",
      "Token loss: 1.9572882652282715\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.6740780842317303\n",
      "Copy loss: 2.474066972732544\n",
      "Token loss: 2.2832720279693604\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.515992467706028\n",
      "Copy loss: 2.490504264831543\n",
      "Token loss: 2.3165106773376465\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.45286505383395087\n",
      "Copy loss: 2.5861082077026367\n",
      "Token loss: 2.6006011962890625\n",
      "-------------------------------------------------------\n",
      "epoch elapsed 172s\n",
      "Action loss: 0.5557797190666636\n",
      "Copy loss: 2.236725091934204\n",
      "Token loss: 2.288270950317383\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.4878043896750082\n",
      "Copy loss: 2.5725903511047363\n",
      "Token loss: 2.2964913845062256\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.6512068545163536\n",
      "Copy loss: 2.4315624237060547\n",
      "Token loss: 2.2528016567230225\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.41295339271040493\n",
      "Copy loss: 2.5214974880218506\n",
      "Token loss: 2.04866886138916\n",
      "-------------------------------------------------------\n",
      "epoch elapsed 257s\n",
      "Action loss: 0.44967410227156357\n",
      "Copy loss: 2.6213488578796387\n",
      "Token loss: 1.596675992012024\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.5804733986732424\n",
      "Copy loss: 2.135213613510132\n",
      "Token loss: 1.9635257720947266\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.4984866529711599\n",
      "Copy loss: 2.2036569118499756\n",
      "Token loss: 1.8015984296798706\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.38928251664979124\n",
      "Copy loss: 1.8524490594863892\n",
      "Token loss: 1.6909881830215454\n",
      "-------------------------------------------------------\n",
      "epoch elapsed 351s\n",
      "Action loss: 0.49314319696137915\n",
      "Copy loss: 2.3268520832061768\n",
      "Token loss: 1.9423855543136597\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.5054309773299818\n",
      "Copy loss: 1.9993597269058228\n",
      "Token loss: 1.94846510887146\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.48465899290231673\n",
      "Copy loss: 2.501537322998047\n",
      "Token loss: 2.105363368988037\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.5118908934590062\n",
      "Copy loss: 1.8679237365722656\n",
      "Token loss: 2.011061668395996\n",
      "-------------------------------------------------------\n",
      "epoch elapsed 437s\n",
      "Action loss: 0.33977529469799755\n",
      "Copy loss: 1.755697250366211\n",
      "Token loss: 1.645026445388794\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.41311723913911935\n",
      "Copy loss: 2.169201374053955\n",
      "Token loss: 1.7432423830032349\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.6023930216430918\n",
      "Copy loss: 1.771520972251892\n",
      "Token loss: 2.043247699737549\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.5409746449992104\n",
      "Copy loss: 2.245431423187256\n",
      "Token loss: 1.5923744440078735\n",
      "-------------------------------------------------------\n",
      "epoch elapsed 521s\n",
      "Action loss: 0.46830570305253566\n",
      "Copy loss: 1.8692893981933594\n",
      "Token loss: 1.3859810829162598\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.5496726419881426\n",
      "Copy loss: 1.9528731107711792\n",
      "Token loss: 1.41001558303833\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.4855105974749982\n",
      "Copy loss: 2.401841878890991\n",
      "Token loss: 1.6335022449493408\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.4282185556929368\n",
      "Copy loss: 2.3505971431732178\n",
      "Token loss: 1.9063302278518677\n",
      "-------------------------------------------------------\n",
      "epoch elapsed 607s\n",
      "Action loss: 0.47147783378997177\n",
      "Copy loss: 1.653273344039917\n",
      "Token loss: 2.045412063598633\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.478088909212386\n",
      "Copy loss: 1.7709782123565674\n",
      "Token loss: 1.289247751235962\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.5150814844779981\n",
      "Copy loss: 1.6476718187332153\n",
      "Token loss: 1.496556043624878\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.6303552340533871\n",
      "Copy loss: 1.508694052696228\n",
      "Token loss: 1.826014518737793\n",
      "-------------------------------------------------------\n",
      "epoch elapsed 694s\n",
      "Action loss: 0.39730974306929556\n",
      "Copy loss: 1.5354161262512207\n",
      "Token loss: 1.550667643547058\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.4556482929672474\n",
      "Copy loss: 1.7133105993270874\n",
      "Token loss: 1.3847991228103638\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.35504979776034845\n",
      "Copy loss: 1.6203173398971558\n",
      "Token loss: 1.462422490119934\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.38822945675101056\n",
      "Copy loss: 1.5216673612594604\n",
      "Token loss: 1.445698618888855\n",
      "-------------------------------------------------------\n",
      "epoch elapsed 779s\n",
      "Action loss: 0.35221128488586834\n",
      "Copy loss: 1.793459177017212\n",
      "Token loss: 1.4606064558029175\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.5079489116776305\n",
      "Copy loss: 1.8279242515563965\n",
      "Token loss: 1.434131145477295\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.34667489930602746\n",
      "Copy loss: 1.6619528532028198\n",
      "Token loss: 1.5155389308929443\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.5250446533046715\n",
      "Copy loss: 1.7671762704849243\n",
      "Token loss: 1.8154865503311157\n",
      "-------------------------------------------------------\n",
      "epoch elapsed 867s\n",
      "Action loss: 0.4414175211326322\n",
      "Copy loss: 1.4495158195495605\n",
      "Token loss: 1.41032874584198\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.4337194228529176\n",
      "Copy loss: 1.811225414276123\n",
      "Token loss: 1.3499596118927002\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.4983613070431844\n",
      "Copy loss: 2.1041743755340576\n",
      "Token loss: 1.2601851224899292\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.3657569052269651\n",
      "Copy loss: 2.075474500656128\n",
      "Token loss: 1.6608595848083496\n",
      "-------------------------------------------------------\n",
      "epoch elapsed 957s\n",
      "Action loss: 0.2392229175696885\n",
      "Copy loss: 1.7346585988998413\n",
      "Token loss: 1.125356912612915\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.38525821580756076\n",
      "Copy loss: 1.9177345037460327\n",
      "Token loss: 1.5059256553649902\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.2560211365626896\n",
      "Copy loss: 1.6320888996124268\n",
      "Token loss: 1.4042550325393677\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.3956803562254734\n",
      "Copy loss: 1.8490307331085205\n",
      "Token loss: 1.5732141733169556\n",
      "-------------------------------------------------------\n",
      "epoch elapsed 1049s\n",
      "Action loss: 0.4238142156193304\n",
      "Copy loss: 2.323441743850708\n",
      "Token loss: 1.0069873332977295\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.29746752938664517\n",
      "Copy loss: 1.5255699157714844\n",
      "Token loss: 1.2510080337524414\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.4613295148922237\n",
      "Copy loss: 1.5787553787231445\n",
      "Token loss: 1.267641544342041\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.48783123504779546\n",
      "Copy loss: 1.5977436304092407\n",
      "Token loss: 1.321461796760559\n",
      "-------------------------------------------------------\n",
      "epoch elapsed 1134s\n",
      "Action loss: 0.43814731163440224\n",
      "Copy loss: 1.645160436630249\n",
      "Token loss: 1.1672691106796265\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.32259112869835455\n",
      "Copy loss: 1.6001461744308472\n",
      "Token loss: 1.2529573440551758\n",
      "-------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action loss: 0.41577124559641504\n",
      "Copy loss: 1.6518689393997192\n",
      "Token loss: 1.20344078540802\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.2549308473095643\n",
      "Copy loss: 1.8392798900604248\n",
      "Token loss: 1.0195257663726807\n",
      "-------------------------------------------------------\n",
      "epoch elapsed 1220s\n",
      "Action loss: 0.3016550911427151\n",
      "Copy loss: 1.5269074440002441\n",
      "Token loss: 0.9525985717773438\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.3523757330724296\n",
      "Copy loss: 1.7236050367355347\n",
      "Token loss: 1.0393800735473633\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.36739460570292143\n",
      "Copy loss: 1.6697598695755005\n",
      "Token loss: 1.332204818725586\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.2840887575819544\n",
      "Copy loss: 1.5513217449188232\n",
      "Token loss: 1.0433471202850342\n",
      "-------------------------------------------------------\n",
      "epoch elapsed 1317s\n",
      "Action loss: 0.3265489389519612\n",
      "Copy loss: 1.7287156581878662\n",
      "Token loss: 0.9686008095741272\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.22503873686004733\n",
      "Copy loss: 1.8579717874526978\n",
      "Token loss: 0.9965659976005554\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.43532010690315953\n",
      "Copy loss: 1.8240398168563843\n",
      "Token loss: 0.9515286087989807\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.33834987412004286\n",
      "Copy loss: 1.430583119392395\n",
      "Token loss: 1.177512288093567\n",
      "-------------------------------------------------------\n",
      "epoch elapsed 1405s\n",
      "Action loss: 0.26341589075677774\n",
      "Copy loss: 1.4736080169677734\n",
      "Token loss: 0.8763882517814636\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.23378161425180863\n",
      "Copy loss: 1.6713987588882446\n",
      "Token loss: 0.7133185863494873\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.4315271449345938\n",
      "Copy loss: 1.4698389768600464\n",
      "Token loss: 0.7505561709403992\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.28893030217586074\n",
      "Copy loss: 1.7940541505813599\n",
      "Token loss: 0.9986572861671448\n",
      "-------------------------------------------------------\n",
      "epoch elapsed 1490s\n",
      "Action loss: 0.30762953592427356\n",
      "Copy loss: 1.5601192712783813\n",
      "Token loss: 0.4965305030345917\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.29204855290060416\n",
      "Copy loss: 1.62592613697052\n",
      "Token loss: 0.7989756464958191\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.30470874970355494\n",
      "Copy loss: 1.6370595693588257\n",
      "Token loss: 0.8428452014923096\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.25575581212160553\n",
      "Copy loss: 1.354950189590454\n",
      "Token loss: 0.7372190952301025\n",
      "-------------------------------------------------------\n",
      "epoch elapsed 1576s\n",
      "Action loss: 0.29611482213310336\n",
      "Copy loss: 1.7328931093215942\n",
      "Token loss: 0.5869181752204895\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.24586192251573796\n",
      "Copy loss: 1.2310776710510254\n",
      "Token loss: 0.5495420098304749\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.36252200680309954\n",
      "Copy loss: 1.5240426063537598\n",
      "Token loss: 0.7690022587776184\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.2574229163186068\n",
      "Copy loss: 1.8911679983139038\n",
      "Token loss: 0.6166386008262634\n",
      "-------------------------------------------------------\n",
      "epoch elapsed 1662s\n",
      "Action loss: 0.2694353571912928\n",
      "Copy loss: 1.7896347045898438\n",
      "Token loss: 0.5794057250022888\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.25539532332794834\n",
      "Copy loss: 1.8519994020462036\n",
      "Token loss: 0.6696787476539612\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.29739137987574155\n",
      "Copy loss: 1.6698157787322998\n",
      "Token loss: 0.6322540044784546\n",
      "-------------------------------------------------------\n",
      "Action loss: 0.24920629663758623\n",
      "Copy loss: 1.677960753440857\n",
      "Token loss: 0.6721696853637695\n",
      "-------------------------------------------------------\n",
      "epoch elapsed 1748s\n"
     ]
    }
   ],
   "source": [
    "epoch_begin = time.time()\n",
    "for e in range(20):\n",
    "    for batch_ind, x in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        (action_logits, action_labels), (copy_logits, copy_labels), (token_logits, token_labels) = model(x)\n",
    "\n",
    "        loss1 = lossFunc(action_logits, action_labels)\n",
    "        loss2 = torch.DoubleTensor([0.0])\n",
    "        if len(copy_logits) > 0:\n",
    "            loss2 = lossFunc(copy_logits, copy_labels)\n",
    "        loss3 = torch.DoubleTensor([0.0])\n",
    "        if len(token_logits) > 0:\n",
    "            loss3 = lossFunc(token_logits, token_labels)\n",
    "\n",
    "        total_loss = loss1 + loss2.double() + loss3.double()\n",
    "        total_loss.backward()\n",
    "\n",
    "        # clip gradient\n",
    "        if hyperParams.clip_grad > 0.:\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), hyperParams.clip_grad)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_ind % hyperParams.log_every == hyperParams.log_every - 1:\n",
    "            print(\"Action loss: {}\".format(loss1.data))\n",
    "            print(\"Copy loss: {}\".format(loss2.data))\n",
    "            print(\"Token loss: {}\".format(loss3.data))\n",
    "            print('-------------------------------------------------------')\n",
    "            report_loss = report_examples = 0.\n",
    "\n",
    "    print('epoch elapsed %ds' % (time.time() - epoch_begin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save((model).state_dict(), 'Parameters/frist.t7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('Parameters/frist.t7'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
